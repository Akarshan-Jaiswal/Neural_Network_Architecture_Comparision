{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "\n",
    "\n",
    "def configure_model(layers_config, optimizer='adam', loss='mean_squared_error', metrics=['accuracy'],debugflag=False):\n",
    "    if debugflag:\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(2,1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        return model\n",
    "        \n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    for layer_config in layers_config:\n",
    "        layer_type = layer_config[0]\n",
    "\n",
    "        if layer_type == 'dense':\n",
    "            neurons, activation = layer_config[1], layer_config[2]\n",
    "            if len(layer_config) > 3 and layer_config[3] == 'input':\n",
    "                input_shape = layer_config[4]\n",
    "                model.add(layers.Dense(neurons, activation=activation, input_shape=(input_shape,)))\n",
    "            else:\n",
    "                model.add(layers.Dense(neurons, activation=activation))\n",
    "        elif layer_type == 'dropout':\n",
    "            rate = layer_config[1]\n",
    "            model.add(layers.Dropout(rate))\n",
    "        elif layer_type == 'activation':\n",
    "            activation = layer_config[1]\n",
    "            model.add(layers.Activation(activation))\n",
    "        elif layer_type == 'conv2d':\n",
    "            filters, kernel_size, strides, activation = layer_config[1], layer_config[2], layer_config[3], layer_config[4]\n",
    "            model.add(layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, activation=activation))\n",
    "        elif layer_type == 'maxpool2d':\n",
    "            pool_size = layer_config[1]\n",
    "            model.add(layers.MaxPool2D(pool_size=pool_size))\n",
    "        elif layer_type == 'rnn':\n",
    "            units, return_sequences = layer_config[1], layer_config[2]\n",
    "            model.add(layers.SimpleRNN(units, return_sequences=return_sequences))\n",
    "        elif layer_type == 'lstm':\n",
    "            units, return_sequences = layer_config[1], layer_config[2]\n",
    "            model.add(layers.LSTM(units, return_sequences=return_sequences))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, model_name='perceptron', dataset_name=\"addition_dataset_2\", total_epoch=50,ea=False,eamonitor=\"val_loss\"):\n",
    "    tensorboard_callback = TensorBoard(log_dir='./../../Observation/'+model_name+'/'+model_name+dataset_name+'logs')\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor=eamonitor, patience=7, min_delta=0.001, verbose=1)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    if ea:\n",
    "        history = model.fit(X_train, y_train, epochs=total_epoch, validation_data=(X_test, y_test), callbacks=[tensorboard_callback,early_stopping])\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, epochs=total_epoch, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    return model, history, predictions, training_time\n",
    "\n",
    "def test_model(model, X_test, y_test, model_name='perceptron', dataset_name=\"addition_dataset_2\",threshold=0.5):\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    metrics = calculate_metrics(predictions, y_test,threshold)\n",
    "    metrics['Training Time'] = training_time\n",
    "    metrics['Inference Time'] = inference_time\n",
    "    metrics['Model type'] = model_name\n",
    "    metrics['dataset'] = dataset_name\n",
    "\n",
    "    print(\"Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    metrics_list = list(metrics.values())\n",
    "    pd.DataFrame([metrics_list], columns=metrics.keys()).to_csv('./../../Observation/'+model_name+'/'+model_name+dataset_name+'metrics.csv', index=False)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_metrics(predictions, true_labels, threshold=0.5):\n",
    "    #threshold = 0.5  # Define the threshold\n",
    "    interpredictions=predictions.flatten()\n",
    "    correct_predictions = np.sum(np.abs(true_labels - interpredictions) <= threshold)\n",
    "    total_predictions = len(true_labels)\n",
    "    accuracy = correct_predictions / total_predictions  # Final accuracy\n",
    "    mse = mean_squared_error(true_labels, interpredictions)  # Mean Squared Error\n",
    "    mae = mean_absolute_error(true_labels, interpredictions)  # Mean Absolute Error\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "    f1 = f1_score(true_labels.round(), interpredictions.round(), average='micro')  # F1 Score\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'F1 Score': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 91458680.0000 - mae: 4641.7783 - val_loss: 0.1382 - val_mae: 0.3093\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1400 - mae: 0.3119 - val_loss: 0.1385 - val_mae: 0.3138\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1415 - mae: 0.3134 - val_loss: 0.1383 - val_mae: 0.3073\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1398 - mae: 0.3107 - val_loss: 0.1377 - val_mae: 0.3139\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1395 - mae: 0.3107 - val_loss: 0.1352 - val_mae: 0.3090\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2147 - mae: 0.3450 - val_loss: 0.1349 - val_mae: 0.3102\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1364 - mae: 0.3083 - val_loss: 0.1366 - val_mae: 0.3150\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1377 - mae: 0.3103 - val_loss: 0.1414 - val_mae: 0.3085\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1346 - mae: 0.3065 - val_loss: 0.1503 - val_mae: 0.3221\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1323 - mae: 0.3041 - val_loss: 0.1470 - val_mae: 0.3314\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1295 - mae: 0.3013 - val_loss: 0.1183 - val_mae: 0.2904\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1257 - mae: 0.2971 - val_loss: 0.1115 - val_mae: 0.2772\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1182 - mae: 0.2871 - val_loss: 0.1045 - val_mae: 0.2724\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1299 - mae: 0.3073 - val_loss: 0.0955 - val_mae: 0.2565\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1140 - mae: 0.2853 - val_loss: 0.0986 - val_mae: 0.2590\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2204 - mae: 0.3656 - val_loss: 0.5583 - val_mae: 0.6108\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 61.5099 - mae: 4.5116 - val_loss: 0.4630 - val_mae: 0.5568\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 566.0491 - mae: 9.9793 - val_loss: 4.7400 - val_mae: 1.8123\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 179.2319 - mae: 4.6522 - val_loss: 9.6970 - val_mae: 2.7990\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 111.2067 - mae: 5.8977 - val_loss: 282.1025 - val_mae: 14.6674\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2160.2041 - mae: 26.4994 - val_loss: 0.0537 - val_mae: 0.1982\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6075 - mae: 0.3720 - val_loss: 2.0806 - val_mae: 1.1980\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1962.2655 - mae: 15.6442 - val_loss: 0.0389 - val_mae: 0.1695\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1425 - mae: 0.2243 - val_loss: 0.0210 - val_mae: 0.1245\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 19.5026 - mae: 0.7359 - val_loss: 431.9049 - val_mae: 17.9783\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1892.9574 - mae: 19.7518 - val_loss: 0.5911 - val_mae: 0.6307\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 721.4481 - mae: 6.3041 - val_loss: 0.0413 - val_mae: 0.2023\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0268 - mae: 0.1373 - val_loss: 0.0153 - val_mae: 0.1072\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.2056 - mae: 1.1599 - val_loss: 38.9180 - val_mae: 5.4664\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 190.0494 - mae: 8.0609 - val_loss: 0.0486 - val_mae: 0.2204\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1388.1031 - mae: 10.5200 - val_loss: 0.0125 - val_mae: 0.0964\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0113 - mae: 0.0914 - val_loss: 0.0140 - val_mae: 0.1145\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3587 - mae: 0.2585 - val_loss: 229.1459 - val_mae: 13.1760\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 998.0692 - mae: 16.8105 - val_loss: 0.2140 - val_mae: 0.3798\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3903.6340 - mae: 22.1428 - val_loss: 0.0053 - val_mae: 0.0662\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0081 - mae: 0.0699 - val_loss: 0.0094 - val_mae: 0.0965\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2086 - mae: 0.1459 - val_loss: 0.0808 - val_mae: 0.2675\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1680.3849 - mae: 22.5949 - val_loss: 0.0012 - val_mae: 0.0295\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0095 - val_mae: 0.0795\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 150.2688 - mae: 4.0591 - val_loss: 70.9936 - val_mae: 7.3217\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 981.9402 - mae: 10.0281 - val_loss: 6.7150e-04 - val_mae: 0.0213\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0011 - mae: 0.0273 - val_loss: 0.0103 - val_mae: 0.0971\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 367.2279 - mae: 3.7044 - val_loss: 3.8113 - val_mae: 1.6909\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8325 - mae: 0.3234 - val_loss: 7.6722e-05 - val_mae: 0.0079\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.7625 - mae: 0.2549 - val_loss: 91.4570 - val_mae: 8.3030\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1111.9006 - mae: 17.3556 - val_loss: 0.0367 - val_mae: 0.1704\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 574.8080 - mae: 5.9397 - val_loss: 9.0987 - val_mae: 2.6087\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7588 - mae: 0.4456 - val_loss: 7.7928e-04 - val_mae: 0.0236\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0087 - mae: 0.0491 - val_loss: 0.0549 - val_mae: 0.1970\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3222.0676 - mae: 21.7900 - val_loss: 5.6648e-04 - val_mae: 0.0233\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step\n",
      "Metrics:\n",
      "Accuracy: 1.0\n",
      "MSE: 0.000566479040065567\n",
      "MAE: 0.02330423612892628\n",
      "RMSE: 0.023800820155313283\n",
      "F1 Score: 1.0\n",
      "Training Time: 29.303192615509033\n",
      "Inference Time: 0.11091113090515137\n",
      "Model type: RNN\n",
      "dataset: addition_dataset_2\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"addition_dataset_2\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=50)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 163441296.0000 - mae: 7511.2637 - val_loss: 0.1703 - val_mae: 0.3443\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1730 - mae: 0.3463 - val_loss: 0.1700 - val_mae: 0.3447\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1758 - mae: 0.3497 - val_loss: 0.1697 - val_mae: 0.3432\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1733 - mae: 0.3466 - val_loss: 0.1691 - val_mae: 0.3434\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 0.1724 - mae: 0.3450 - val_loss: 0.1689 - val_mae: 0.3439\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1718 - mae: 0.3453 - val_loss: 0.1691 - val_mae: 0.3466\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1716 - mae: 0.3457 - val_loss: 0.1677 - val_mae: 0.3449\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1710 - mae: 0.3447 - val_loss: 0.1741 - val_mae: 0.3422\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1686 - mae: 0.3411 - val_loss: 0.1635 - val_mae: 0.3372\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1687 - mae: 0.3428 - val_loss: 0.1636 - val_mae: 0.3337\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 0.1705 - mae: 0.3456 - val_loss: 0.1621 - val_mae: 0.3422\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 0.1636 - mae: 0.3368 - val_loss: 0.1636 - val_mae: 0.3313\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1626 - mae: 0.3375 - val_loss: 0.1612 - val_mae: 0.3289\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1572 - mae: 0.3306 - val_loss: 0.1444 - val_mae: 0.3143\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1490 - mae: 0.3215 - val_loss: 0.1845 - val_mae: 0.3745\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1479 - mae: 0.3227 - val_loss: 0.1549 - val_mae: 0.3288\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1413 - mae: 0.3163 - val_loss: 0.1206 - val_mae: 0.2958\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1342 - mae: 0.3087 - val_loss: 0.1047 - val_mae: 0.2721\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1357 - mae: 0.3135 - val_loss: 0.1114 - val_mae: 0.2795\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1230 - mae: 0.2988 - val_loss: 0.0846 - val_mae: 0.2385\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2800 - mae: 0.3744 - val_loss: 2.1552 - val_mae: 1.2036\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 1243.2905 - mae: 14.4666 - val_loss: 0.0532 - val_mae: 0.1978\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0860 - mae: 0.2455 - val_loss: 0.0533 - val_mae: 0.1979\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2216 - mae: 0.3312 - val_loss: 0.1270 - val_mae: 0.2958\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 190.0742 - mae: 5.5304 - val_loss: 0.0284 - val_mae: 0.1399\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.6805 - mae: 1.9167 - val_loss: 3.5560 - val_mae: 1.5795\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 46.6924 - mae: 2.6647 - val_loss: 357.0717 - val_mae: 16.4640\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 136.3856 - mae: 5.3205 - val_loss: 1.8672 - val_mae: 1.1442\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1248.9043 - mae: 12.1750 - val_loss: 0.0163 - val_mae: 0.1054\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0216 - mae: 0.1262 - val_loss: 0.0119 - val_mae: 0.0898\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 0.0278 - mae: 0.1303 - val_loss: 0.0507 - val_mae: 0.2244\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 290.2451 - mae: 6.0033 - val_loss: 0.0290 - val_mae: 0.1416\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 94.4259 - mae: 2.8276 - val_loss: 0.0549 - val_mae: 0.1914\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 32.0119 - mae: 1.9690 - val_loss: 2.0971 - val_mae: 1.2289\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 240.8123 - mae: 7.6969 - val_loss: 30.7512 - val_mae: 4.8418\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 320.1388 - mae: 9.4602 - val_loss: 6.8537 - val_mae: 2.2475\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 301.7980 - mae: 9.3805 - val_loss: 0.0312 - val_mae: 0.1699\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 44.8976 - mae: 2.4430 - val_loss: 3308.7979 - val_mae: 49.9129\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3082.9336 - mae: 22.9786 - val_loss: 1.1474e-04 - val_mae: 0.0089\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.5887e-04 - mae: 0.0106 - val_loss: 1.2581e-04 - val_mae: 0.0102\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 0.0019 - mae: 0.0288 - val_loss: 1.2043e-04 - val_mae: 0.0103\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - loss: 240.2026 - mae: 2.8754 - val_loss: 0.0021 - val_mae: 0.0419\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 984us/step - loss: 0.0181 - mae: 0.0856 - val_loss: 0.0792 - val_mae: 0.2333\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982us/step - loss: 181.5087 - mae: 5.0480 - val_loss: 0.0067 - val_mae: 0.0674\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.4250 - mae: 0.4638 - val_loss: 1369.8135 - val_mae: 32.1438\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982us/step - loss: 754.3424 - mae: 12.9470 - val_loss: 0.0102 - val_mae: 0.0998\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 200.3225 - mae: 2.5868 - val_loss: 0.0259 - val_mae: 0.1530\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0514 - val_loss: 0.0037 - val_mae: 0.0503\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 984us/step - loss: 77.7197 - mae: 1.4666 - val_loss: 0.0066 - val_mae: 0.0666\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4122 - mae: 0.1844 - val_loss: 1.2576 - val_mae: 0.9859\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step\n",
      "Metrics:\n",
      "Accuracy: 0.2375\n",
      "MSE: 1.257609906578936\n",
      "MAE: 0.985883578825742\n",
      "RMSE: 1.1214320784510028\n",
      "F1 Score: 0.23750000000000002\n",
      "Training Time: 27.51389169692993\n",
      "Inference Time: 0.11756515502929688\n",
      "Model type: RNN\n",
      "dataset: doubling_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"doubling_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=50)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8795022.0000 - mae: 1060.8656 - val_loss: 0.0047 - val_mae: 0.0574\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 0.0048 - mae: 0.0573 - val_loss: 0.0047 - val_mae: 0.0574\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0048 - mae: 0.0578 - val_loss: 0.0047 - val_mae: 0.0568\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 0.0048 - mae: 0.0578 - val_loss: 0.0047 - val_mae: 0.0577\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 0.0047 - mae: 0.0573 - val_loss: 0.0046 - val_mae: 0.0569\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 0.0047 - mae: 0.0570 - val_loss: 0.0046 - val_mae: 0.0561\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0574 - val_loss: 0.0045 - val_mae: 0.0569\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0564 - val_loss: 0.0045 - val_mae: 0.0553\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0567 - val_loss: 0.0044 - val_mae: 0.0548\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.0046 - mae: 0.0565 - val_loss: 0.0075 - val_mae: 0.0823\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 0.0046 - mae: 0.0571 - val_loss: 0.0043 - val_mae: 0.0537\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 0.0046 - mae: 0.0569 - val_loss: 0.0099 - val_mae: 0.0982\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0584 - val_loss: 0.0037 - val_mae: 0.0497\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0043 - mae: 0.0557 - val_loss: 0.0032 - val_mae: 0.0475\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 0.0120 - mae: 0.0780 - val_loss: 0.2126 - val_mae: 0.3819\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 101.7423 - mae: 2.5122 - val_loss: 9.7072 - val_mae: 2.6759\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 10.8659 - mae: 0.7996 - val_loss: 0.1121 - val_mae: 0.2739\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 800.9580 - mae: 14.5406 - val_loss: 0.6848 - val_mae: 0.7453\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 51.4408 - mae: 2.0598 - val_loss: 3.9533 - val_mae: 1.6958\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 16.3873 - mae: 1.9061 - val_loss: 11220.2910 - val_mae: 91.9739\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 913.2927 - mae: 15.7882 - val_loss: 0.0037 - val_mae: 0.0527\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 191.2319 - mae: 4.6748 - val_loss: 0.9560 - val_mae: 0.8747\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0843 - mae: 0.1637 - val_loss: 0.0742 - val_mae: 0.2227\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 678.3915 - mae: 8.0536 - val_loss: 0.0028 - val_mae: 0.0454\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0039 - mae: 0.0503 - val_loss: 0.0171 - val_mae: 0.1066\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 35.6228 - mae: 1.6350 - val_loss: 912.1440 - val_mae: 26.2315\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 799.0815 - mae: 17.1351 - val_loss: 5.6694e-04 - val_mae: 0.0204\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0017 - mae: 0.0318 - val_loss: 0.2600 - val_mae: 0.4335\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 407.4576 - mae: 10.5097 - val_loss: 0.0144 - val_mae: 0.0983\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.9668 - mae: 3.8334 - val_loss: 599.4905 - val_mae: 21.2632\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 233.3725 - mae: 8.9399 - val_loss: 3528.4375 - val_mae: 51.5709\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1649.4369 - mae: 18.2926 - val_loss: 0.0069 - val_mae: 0.0803\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0011 - mae: 0.0275 - val_loss: 3.6960e-04 - val_mae: 0.0163\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 210.4928 - mae: 3.7238 - val_loss: 4.7353e-04 - val_mae: 0.0179\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24.2387 - mae: 0.5928 - val_loss: 22.7657 - val_mae: 4.1293\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 55.7573 - mae: 4.4261 - val_loss: 0.0014 - val_mae: 0.0321\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 757.3724 - mae: 7.3888 - val_loss: 0.0014 - val_mae: 0.0322\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0011 - mae: 0.0279 - val_loss: 0.0018 - val_mae: 0.0356\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0010 - mae: 0.0273 - val_loss: 6.4095e-04 - val_mae: 0.0217\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 39.3344 - mae: 0.7205 - val_loss: 157.8757 - val_mae: 10.8928\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.0141 - mae: 1.4460 - val_loss: 6.2874e-04 - val_mae: 0.0212\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 67.2441 - mae: 0.7992 - val_loss: 734.1732 - val_mae: 23.5349\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 31.4645 - mae: 2.1509 - val_loss: 9.3662e-04 - val_mae: 0.0250\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0011 - mae: 0.0287 - val_loss: 7.7050e-04 - val_mae: 0.0231\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0017 - mae: 0.0340 - val_loss: 0.0031 - val_mae: 0.0554\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 105.1170 - mae: 1.9629 - val_loss: 0.0018 - val_mae: 0.0428\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 187.3691 - mae: 7.5516 - val_loss: 5.3605e-04 - val_mae: 0.0199\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0636 - mae: 0.0845 - val_loss: 0.0706 - val_mae: 0.2231\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 156.7233 - mae: 3.6330 - val_loss: 0.0034 - val_mae: 0.0578\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.6181 - mae: 1.3676 - val_loss: 1.4820 - val_mae: 1.0489\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step\n",
      "Metrics:\n",
      "Accuracy: 0.24325\n",
      "MSE: 1.4819605165512644\n",
      "MAE: 1.0489233262371271\n",
      "RMSE: 1.2173580067306677\n",
      "F1 Score: 0.24175\n",
      "Training Time: 27.553702116012573\n",
      "Inference Time: 0.11743569374084473\n",
      "Model type: RNN\n",
      "dataset: substraction_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"substraction_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=50)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 343403.5938 - mae: 207.8793 - val_loss: 0.5087 - val_mae: 0.6439\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5068 - mae: 0.6363 - val_loss: 0.5151 - val_mae: 0.6452\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5142 - mae: 0.6409 - val_loss: 0.5375 - val_mae: 0.6529\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5200 - mae: 0.6421 - val_loss: 0.5073 - val_mae: 0.6435\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5356 - mae: 0.6451 - val_loss: 0.5302 - val_mae: 0.6506\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5421 - mae: 0.6497 - val_loss: 0.5315 - val_mae: 0.6496\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5741 - mae: 0.6585 - val_loss: 0.5080 - val_mae: 0.6434\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 973us/step - loss: 0.5952 - mae: 0.6670 - val_loss: 0.9078 - val_mae: 0.7808\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6522 - mae: 0.6870 - val_loss: 0.7171 - val_mae: 0.7082\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7813 - mae: 0.7343 - val_loss: 0.5678 - val_mae: 0.6620\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.0150 - mae: 0.8172 - val_loss: 0.7364 - val_mae: 0.7146\n",
      "Epoch 11: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step\n",
      "Metrics:\n",
      "Accuracy: 0.39125\n",
      "MSE: 0.7363631719121213\n",
      "MAE: 0.7146358514204728\n",
      "RMSE: 0.8581160596982912\n",
      "F1 Score: 0.32675\n",
      "Training Time: 6.760465860366821\n",
      "Inference Time: 0.12850713729858398\n",
      "Model type: RNN\n",
      "dataset: simple_sin_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"simple_sin_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=50,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 49648.0469 - mae: 74.8407 - val_loss: 0.4953 - val_mae: 0.6339\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5161 - mae: 0.6420 - val_loss: 0.4952 - val_mae: 0.6339\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5499 - mae: 0.6568 - val_loss: 0.4960 - val_mae: 0.6342\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5416 - mae: 0.6520 - val_loss: 0.5767 - val_mae: 0.6580\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5467 - mae: 0.6491 - val_loss: 0.4965 - val_mae: 0.6343\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5616 - mae: 0.6548 - val_loss: 0.4994 - val_mae: 0.6347\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7003 - mae: 0.7061 - val_loss: 0.5751 - val_mae: 0.6598\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6621 - mae: 0.6934 - val_loss: 0.6984 - val_mae: 0.7015\n",
      "Epoch 8: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step\n",
      "Metrics:\n",
      "Accuracy: 0.39775\n",
      "MSE: 0.698357814445811\n",
      "MAE: 0.7014733898176055\n",
      "RMSE: 0.8356780566975605\n",
      "F1 Score: 0.3345\n",
      "Training Time: 5.178182125091553\n",
      "Inference Time: 0.11120438575744629\n",
      "Model type: RNN\n",
      "dataset: simple_cos_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"simple_cos_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=50,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 30215706960199680.0000 - mae: 128070336.0000 - val_loss: 4739719212564480.0000 - val_mae: 47643608.0000\n",
      "Epoch 2/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2672290265300992.0000 - mae: 41189996.0000 - val_loss: 1997759343230976.0000 - val_mae: 39665716.0000\n",
      "Epoch 3/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2039165646536704.0000 - mae: 39968628.0000 - val_loss: 1998104953880576.0000 - val_mae: 39722260.0000\n",
      "Epoch 4/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1973160320696320.0000 - mae: 39253952.0000 - val_loss: 2007960964300800.0000 - val_mae: 40056712.0000\n",
      "Epoch 5/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2002068269170688.0000 - mae: 39638792.0000 - val_loss: 1998907978547200.0000 - val_mae: 39787564.0000\n",
      "Epoch 6/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2008579573809152.0000 - mae: 39697340.0000 - val_loss: 2002283554406400.0000 - val_mae: 39915268.0000\n",
      "Epoch 7/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997352126644224.0000 - mae: 39521736.0000 - val_loss: 2003138387116032.0000 - val_mae: 39942864.0000\n",
      "Epoch 8/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1998805704638464.0000 - mae: 39602460.0000 - val_loss: 1997074564382720.0000 - val_mae: 39705772.0000\n",
      "Epoch 9/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997293339279360.0000 - mae: 39518648.0000 - val_loss: 1997297768464384.0000 - val_mae: 39509704.0000\n",
      "Epoch 10/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1993734422003712.0000 - mae: 39388660.0000 - val_loss: 1997637205098496.0000 - val_mae: 39761112.0000\n",
      "Epoch 11/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2011864418484224.0000 - mae: 39689952.0000 - val_loss: 2003032623546368.0000 - val_mae: 39951968.0000\n",
      "Epoch 12/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 2009631169708032.0000 - mae: 39764832.0000 - val_loss: 1998122670620672.0000 - val_mae: 39447660.0000\n",
      "Epoch 13/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2036198763659264.0000 - mae: 39891880.0000 - val_loss: 1995582734336000.0000 - val_mae: 39629048.0000\n",
      "Epoch 14/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2011522968584192.0000 - mae: 39621280.0000 - val_loss: 1996267378966528.0000 - val_mae: 39718088.0000\n",
      "Epoch 15/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2004212800028672.0000 - mae: 39547292.0000 - val_loss: 1995274301997056.0000 - val_mae: 39562932.0000\n",
      "Epoch 16/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1996735798837248.0000 - mae: 39509956.0000 - val_loss: 1994964259045376.0000 - val_mae: 39589408.0000\n",
      "Epoch 17/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 2004472645550080.0000 - mae: 39621404.0000 - val_loss: 1995664741367808.0000 - val_mae: 39494876.0000\n",
      "Epoch 18/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2045025324105728.0000 - mae: 40089424.0000 - val_loss: 2002362205995008.0000 - val_mae: 39954288.0000\n",
      "Epoch 19/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2025039230664704.0000 - mae: 39923412.0000 - val_loss: 2012687441592320.0000 - val_mae: 40181816.0000\n",
      "Epoch 20/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 2032997536628736.0000 - mae: 39999144.0000 - val_loss: 1997717333082112.0000 - val_mae: 39823796.0000\n",
      "Epoch 21/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2001434358841344.0000 - mae: 39561140.0000 - val_loss: 1994286459518976.0000 - val_mae: 39526812.0000\n",
      "Epoch 22/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2007527977910272.0000 - mae: 39598832.0000 - val_loss: 1995221822865408.0000 - val_mae: 39458076.0000\n",
      "Epoch 23/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1987435449810944.0000 - mae: 39294596.0000 - val_loss: 1996332877217792.0000 - val_mae: 39786916.0000\n",
      "Epoch 24/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2023815433420800.0000 - mae: 39831844.0000 - val_loss: 2013338666008576.0000 - val_mae: 40201276.0000\n",
      "Epoch 25/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 2008164438376448.0000 - mae: 39683192.0000 - val_loss: 1995074049146880.0000 - val_mae: 39745048.0000\n",
      "Epoch 26/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1987921317986304.0000 - mae: 39420368.0000 - val_loss: 1994677435760640.0000 - val_mae: 39733020.0000\n",
      "Epoch 27/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 2007995458256896.0000 - mae: 39620268.0000 - val_loss: 1993209228034048.0000 - val_mae: 39649300.0000\n",
      "Epoch 28/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1995404090540032.0000 - mae: 39452152.0000 - val_loss: 2000470675554304.0000 - val_mae: 39933096.0000\n",
      "Epoch 29/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2018397869047808.0000 - mae: 39768916.0000 - val_loss: 1994923188420608.0000 - val_mae: 39763868.0000\n",
      "Epoch 30/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2005074075189248.0000 - mae: 39634880.0000 - val_loss: 1992332517834752.0000 - val_mae: 39608924.0000\n",
      "Epoch 31/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1983714129084416.0000 - mae: 39425108.0000 - val_loss: 1993372436791296.0000 - val_mae: 39705140.0000\n",
      "Epoch 32/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 1977914178404352.0000 - mae: 39271028.0000 - val_loss: 1992056297750528.0000 - val_mae: 39620972.0000\n",
      "Epoch 33/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 2009589025341440.0000 - mae: 39652060.0000 - val_loss: 1991599823257600.0000 - val_mae: 39557996.0000\n",
      "Epoch 34/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 1998599411990528.0000 - mae: 39465292.0000 - val_loss: 1993925145395200.0000 - val_mae: 39752120.0000\n",
      "Epoch 35/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1990779350286336.0000 - mae: 39413264.0000 - val_loss: 1994110768513024.0000 - val_mae: 39765152.0000\n",
      "Epoch 36/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1967128307564544.0000 - mae: 39194800.0000 - val_loss: 2013665888829440.0000 - val_mae: 40223208.0000\n",
      "Epoch 37/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1995466233348096.0000 - mae: 39594148.0000 - val_loss: 1991292733095936.0000 - val_mae: 39633064.0000\n",
      "Epoch 38/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000us/step - loss: 1971355360690176.0000 - mae: 39187060.0000 - val_loss: 1992323391029248.0000 - val_mae: 39706896.0000\n",
      "Epoch 39/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 1993157151555584.0000 - mae: 39440480.0000 - val_loss: 1994285788430336.0000 - val_mae: 39791260.0000\n",
      "Epoch 40/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2002165442805760.0000 - mae: 39651132.0000 - val_loss: 1990446758756352.0000 - val_mae: 39602064.0000\n",
      "Epoch 41/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 2005273388515328.0000 - mae: 39582992.0000 - val_loss: 1990016188284928.0000 - val_mae: 39521516.0000\n",
      "Epoch 42/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 2018913533558784.0000 - mae: 39708020.0000 - val_loss: 1989786139099136.0000 - val_mae: 39531932.0000\n",
      "Epoch 43/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 1986655107940352.0000 - mae: 39448456.0000 - val_loss: 1999227819393024.0000 - val_mae: 39941848.0000\n",
      "Epoch 44/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1989630043881472.0000 - mae: 39437720.0000 - val_loss: 1995397782306816.0000 - val_mae: 39844904.0000\n",
      "Epoch 45/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1988153514655744.0000 - mae: 39500520.0000 - val_loss: 1993263183560704.0000 - val_mae: 39783524.0000\n",
      "Epoch 46/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 2016711423295488.0000 - mae: 39909536.0000 - val_loss: 1989389257277440.0000 - val_mae: 39457160.0000\n",
      "Epoch 47/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1986160918265856.0000 - mae: 39314340.0000 - val_loss: 1996712042299392.0000 - val_mae: 39889908.0000\n",
      "Epoch 48/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 2009518695251968.0000 - mae: 39703580.0000 - val_loss: 1988694680535040.0000 - val_mae: 39481120.0000\n",
      "Epoch 49/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2005799253573632.0000 - mae: 39580732.0000 - val_loss: 1990134568321024.0000 - val_mae: 39683452.0000\n",
      "Epoch 50/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2013762794029056.0000 - mae: 39678732.0000 - val_loss: 1988844735954944.0000 - val_mae: 39618584.0000\n",
      "Epoch 51/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 1999604165902336.0000 - mae: 39540936.0000 - val_loss: 1987930579009536.0000 - val_mae: 39526792.0000\n",
      "Epoch 52/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1978518695051264.0000 - mae: 39215128.0000 - val_loss: 1997528757174272.0000 - val_mae: 39923792.0000\n",
      "Epoch 53/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997602845360128.0000 - mae: 39600544.0000 - val_loss: 1991200525516800.0000 - val_mae: 39750360.0000\n",
      "Epoch 54/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2004035229974528.0000 - mae: 39607648.0000 - val_loss: 2001326447788032.0000 - val_mae: 40012656.0000\n",
      "Epoch 55/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 1978276834705408.0000 - mae: 39311804.0000 - val_loss: 1996579972055040.0000 - val_mae: 39908760.0000\n",
      "Epoch 56/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 1986562095054848.0000 - mae: 39399540.0000 - val_loss: 1987143123599360.0000 - val_mae: 39564992.0000\n",
      "Epoch 57/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 2011102732877824.0000 - mae: 39673992.0000 - val_loss: 1989260139823104.0000 - val_mae: 39699252.0000\n",
      "Epoch 58/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1991795646922752.0000 - mae: 39489080.0000 - val_loss: 1986448278421504.0000 - val_mae: 39509212.0000\n",
      "Epoch 59/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1992644976705536.0000 - mae: 39440272.0000 - val_loss: 1986620748201984.0000 - val_mae: 39569776.0000\n",
      "Epoch 60/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1980914884149248.0000 - mae: 39306476.0000 - val_loss: 1991513923911680.0000 - val_mae: 39790168.0000\n",
      "Epoch 61/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1981871722332160.0000 - mae: 39372332.0000 - val_loss: 1987499471667200.0000 - val_mae: 39353056.0000\n",
      "Epoch 62/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1987989903245312.0000 - mae: 39318060.0000 - val_loss: 2002250268409856.0000 - val_mae: 40046196.0000\n",
      "Epoch 63/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1992822815195136.0000 - mae: 39573716.0000 - val_loss: 1992039252099072.0000 - val_mae: 39232192.0000\n",
      "Epoch 64/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 2004575322112000.0000 - mae: 39526884.0000 - val_loss: 2001013988917248.0000 - val_mae: 40025064.0000\n",
      "Epoch 65/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 1991726122139648.0000 - mae: 39550128.0000 - val_loss: 1992332920487936.0000 - val_mae: 39830316.0000\n",
      "Epoch 66/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974us/step - loss: 2004588341231616.0000 - mae: 39704496.0000 - val_loss: 1985593848365056.0000 - val_mae: 39590852.0000\n",
      "Epoch 67/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2010151397621760.0000 - mae: 39657080.0000 - val_loss: 1984506147897344.0000 - val_mae: 39496240.0000\n",
      "Epoch 68/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 965us/step - loss: 1987228083421184.0000 - mae: 39349172.0000 - val_loss: 1986180648271872.0000 - val_mae: 39641532.0000\n",
      "Epoch 69/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 2000646366560256.0000 - mae: 39638760.0000 - val_loss: 1986820598398976.0000 - val_mae: 39675740.0000\n",
      "Epoch 70/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 2005859919986688.0000 - mae: 39648180.0000 - val_loss: 1984102957842432.0000 - val_mae: 39531652.0000\n",
      "Epoch 71/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 1999831664951296.0000 - mae: 39590532.0000 - val_loss: 1983654536413184.0000 - val_mae: 39432508.0000\n",
      "Epoch 72/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1998534450610176.0000 - mae: 39526336.0000 - val_loss: 1990398171938816.0000 - val_mae: 39802024.0000\n",
      "Epoch 73/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999us/step - loss: 2007928215175168.0000 - mae: 39666228.0000 - val_loss: 1989893513281536.0000 - val_mae: 39791832.0000\n",
      "Epoch 74/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 1990067996327936.0000 - mae: 39483280.0000 - val_loss: 1983463276150784.0000 - val_mae: 39546220.0000\n",
      "Epoch 75/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1973730746040320.0000 - mae: 39249308.0000 - val_loss: 1992660680179712.0000 - val_mae: 39868520.0000\n",
      "Epoch 76/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1992347281784832.0000 - mae: 39472136.0000 - val_loss: 1984343073357824.0000 - val_mae: 39619492.0000\n",
      "Epoch 77/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1975055475015680.0000 - mae: 39318212.0000 - val_loss: 1989521595957248.0000 - val_mae: 39795860.0000\n",
      "Epoch 78/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1987326733451264.0000 - mae: 39435164.0000 - val_loss: 1985757057122304.0000 - val_mae: 39688288.0000\n",
      "Epoch 79/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1991102143922176.0000 - mae: 39346480.0000 - val_loss: 1981720324734976.0000 - val_mae: 39449804.0000\n",
      "Epoch 80/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997545400172544.0000 - mae: 39511436.0000 - val_loss: 1982027549114368.0000 - val_mae: 39359368.0000\n",
      "Epoch 81/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1996583193280512.0000 - mae: 39573100.0000 - val_loss: 1982140292005888.0000 - val_mae: 39549988.0000\n",
      "Epoch 82/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1992108105793536.0000 - mae: 39446828.0000 - val_loss: 1982460401287168.0000 - val_mae: 39305868.0000\n",
      "Epoch 83/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1974417001283584.0000 - mae: 39214580.0000 - val_loss: 1981541144068096.0000 - val_mae: 39536292.0000\n",
      "Epoch 84/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 1979084422774784.0000 - mae: 39282348.0000 - val_loss: 1980682419044352.0000 - val_mae: 39475576.0000\n",
      "Epoch 85/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1999996215885824.0000 - mae: 39508028.0000 - val_loss: 1989805869105152.0000 - val_mae: 39829012.0000\n",
      "Epoch 86/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1979773630808064.0000 - mae: 39379344.0000 - val_loss: 1983735067049984.0000 - val_mae: 39226004.0000\n",
      "Epoch 87/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1973454660173824.0000 - mae: 39185092.0000 - val_loss: 1980109040910336.0000 - val_mae: 39356992.0000\n",
      "Epoch 88/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2016483655811072.0000 - mae: 39704104.0000 - val_loss: 1980961323483136.0000 - val_mae: 39560540.0000\n",
      "Epoch 89/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2017006433861632.0000 - mae: 39802988.0000 - val_loss: 1979424933150720.0000 - val_mae: 39371868.0000\n",
      "Epoch 90/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 2005426665160704.0000 - mae: 39639032.0000 - val_loss: 1985044092551168.0000 - val_mae: 39722104.0000\n",
      "Epoch 91/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1958089951543296.0000 - mae: 39078472.0000 - val_loss: 1979645452877824.0000 - val_mae: 39518668.0000\n",
      "Epoch 92/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1980599069835264.0000 - mae: 39215908.0000 - val_loss: 1983327984680960.0000 - val_mae: 39680360.0000\n",
      "Epoch 93/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2001952573489152.0000 - mae: 39626252.0000 - val_loss: 1981412831920128.0000 - val_mae: 39621064.0000\n",
      "Epoch 94/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 1979061471543296.0000 - mae: 39229544.0000 - val_loss: 1978566073909248.0000 - val_mae: 39486896.0000\n",
      "Epoch 95/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974us/step - loss: 1957021175775232.0000 - mae: 39023708.0000 - val_loss: 1978712505450496.0000 - val_mae: 39289980.0000\n",
      "Epoch 96/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 1983341943324672.0000 - mae: 39306440.0000 - val_loss: 1985597472243712.0000 - val_mae: 39760608.0000\n",
      "Epoch 97/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1990572789202944.0000 - mae: 39426240.0000 - val_loss: 1978105707102208.0000 - val_mae: 39501120.0000\n",
      "Epoch 98/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 1993036624035840.0000 - mae: 39354624.0000 - val_loss: 1977501190455296.0000 - val_mae: 39306188.0000\n",
      "Epoch 99/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1991268573904896.0000 - mae: 39462972.0000 - val_loss: 1977536221282304.0000 - val_mae: 39492192.0000\n",
      "Epoch 100/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 1978920945582080.0000 - mae: 39238000.0000 - val_loss: 1983050825072640.0000 - val_mae: 39708372.0000\n",
      "Epoch 101/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997311324454912.0000 - mae: 39542288.0000 - val_loss: 1979711085346816.0000 - val_mae: 39183404.0000\n",
      "Epoch 102/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 998us/step - loss: 1970343761674240.0000 - mae: 39113848.0000 - val_loss: 1976418456043520.0000 - val_mae: 39459940.0000\n",
      "Epoch 103/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 1986422642835456.0000 - mae: 39362064.0000 - val_loss: 1975989093531648.0000 - val_mae: 39443168.0000\n",
      "Epoch 104/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1964411874967552.0000 - mae: 39135272.0000 - val_loss: 1978088527233024.0000 - val_mae: 39571424.0000\n",
      "Epoch 105/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1984030882922496.0000 - mae: 39342748.0000 - val_loss: 1977299461210112.0000 - val_mae: 39547004.0000\n",
      "Epoch 106/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1984238517747712.0000 - mae: 39321528.0000 - val_loss: 1986974814568448.0000 - val_mae: 39825312.0000\n",
      "Epoch 107/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1982553414172672.0000 - mae: 39398696.0000 - val_loss: 1979396613210112.0000 - val_mae: 39635832.0000\n",
      "Epoch 108/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1972705591033856.0000 - mae: 39238032.0000 - val_loss: 1977111287955456.0000 - val_mae: 39176644.0000\n",
      "Epoch 109/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1986933341290496.0000 - mae: 39332992.0000 - val_loss: 1974729057501184.0000 - val_mae: 39252128.0000\n",
      "Epoch 110/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1941949330227200.0000 - mae: 38805368.0000 - val_loss: 1984878870528000.0000 - val_mae: 39790440.0000\n",
      "Epoch 111/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 1954847922323456.0000 - mae: 38997340.0000 - val_loss: 1974303453085696.0000 - val_mae: 39460976.0000\n",
      "Epoch 112/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 1989231283011584.0000 - mae: 39433384.0000 - val_loss: 1973157099470848.0000 - val_mae: 39380608.0000\n",
      "Epoch 113/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1992280978227200.0000 - mae: 39421392.0000 - val_loss: 1973674911465472.0000 - val_mae: 39231592.0000\n",
      "Epoch 114/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1988571334443008.0000 - mae: 39453628.0000 - val_loss: 1972645864144896.0000 - val_mae: 39280772.0000\n",
      "Epoch 115/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1974868241285120.0000 - mae: 39263568.0000 - val_loss: 1972529094721536.0000 - val_mae: 39259092.0000\n",
      "Epoch 116/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1962456456888320.0000 - mae: 39008924.0000 - val_loss: 1980277752594432.0000 - val_mae: 39702408.0000\n",
      "Epoch 117/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1973762287206400.0000 - mae: 39287388.0000 - val_loss: 1983520318685184.0000 - val_mae: 39782360.0000\n",
      "Epoch 118/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 1992636252553216.0000 - mae: 39536248.0000 - val_loss: 1971249328685056.0000 - val_mae: 39291076.0000\n",
      "Epoch 119/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 1983767145086976.0000 - mae: 39226088.0000 - val_loss: 1971272682569728.0000 - val_mae: 39387744.0000\n",
      "Epoch 120/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1959127320363008.0000 - mae: 38946408.0000 - val_loss: 1970571126505472.0000 - val_mae: 39297564.0000\n",
      "Epoch 121/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1952327984480256.0000 - mae: 38888392.0000 - val_loss: 1975174794575872.0000 - val_mae: 39586032.0000\n",
      "Epoch 122/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1970015330893824.0000 - mae: 39212672.0000 - val_loss: 1970796075417600.0000 - val_mae: 39414288.0000\n",
      "Epoch 123/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982us/step - loss: 1997621501624320.0000 - mae: 39493632.0000 - val_loss: 1971909814124544.0000 - val_mae: 39485120.0000\n",
      "Epoch 124/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1993247882739712.0000 - mae: 39471248.0000 - val_loss: 1970077742137344.0000 - val_mae: 39402812.0000\n",
      "Epoch 125/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1990154835197952.0000 - mae: 39458712.0000 - val_loss: 1969142781444096.0000 - val_mae: 39346264.0000\n",
      "Epoch 126/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1959013101076480.0000 - mae: 39171036.0000 - val_loss: 1976382485692416.0000 - val_mae: 39647172.0000\n",
      "Epoch 127/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 1971835054850048.0000 - mae: 39256216.0000 - val_loss: 1971591047020544.0000 - val_mae: 39509748.0000\n",
      "Epoch 128/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1968076824248320.0000 - mae: 39205440.0000 - val_loss: 1968068234313728.0000 - val_mae: 39230584.0000\n",
      "Epoch 129/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1974883005235200.0000 - mae: 39261936.0000 - val_loss: 1968877835649024.0000 - val_mae: 39413672.0000\n",
      "Epoch 130/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 1963264313393152.0000 - mae: 39055372.0000 - val_loss: 1986678864478208.0000 - val_mae: 39883748.0000\n",
      "Epoch 131/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 1963447654809600.0000 - mae: 39253456.0000 - val_loss: 1968023002939392.0000 - val_mae: 39397276.0000\n",
      "Epoch 132/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 1973256286371840.0000 - mae: 39208028.0000 - val_loss: 1966684986408960.0000 - val_mae: 39317128.0000\n",
      "Epoch 133/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1986679401349120.0000 - mae: 39326612.0000 - val_loss: 1971064913526784.0000 - val_mae: 39537884.0000\n",
      "Epoch 134/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 1995285039415296.0000 - mae: 39594568.0000 - val_loss: 1966429570072576.0000 - val_mae: 39161500.0000\n",
      "Epoch 135/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1958479854043136.0000 - mae: 38968232.0000 - val_loss: 1967805972873216.0000 - val_mae: 39440444.0000\n",
      "Epoch 136/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1969538321088512.0000 - mae: 39131216.0000 - val_loss: 1966205829120000.0000 - val_mae: 39377244.0000\n",
      "Epoch 137/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1976976533356544.0000 - mae: 39249008.0000 - val_loss: 1979055297527808.0000 - val_mae: 39753236.0000\n",
      "Epoch 138/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 1969859101458432.0000 - mae: 39242044.0000 - val_loss: 1969124125179904.0000 - val_mae: 39514100.0000\n",
      "Epoch 139/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1967800469946368.0000 - mae: 39179264.0000 - val_loss: 1964089215549440.0000 - val_mae: 39292944.0000\n",
      "Epoch 140/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1989101897121792.0000 - mae: 39388736.0000 - val_loss: 1965883035484160.0000 - val_mae: 39418356.0000\n",
      "Epoch 141/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1959624194392064.0000 - mae: 39052360.0000 - val_loss: 1969017287868416.0000 - val_mae: 39532024.0000\n",
      "Epoch 142/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1961372246081536.0000 - mae: 39132388.0000 - val_loss: 1962833877139456.0000 - val_mae: 39160952.0000\n",
      "Epoch 143/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1963235859234816.0000 - mae: 39117236.0000 - val_loss: 1967756714967040.0000 - val_mae: 39509384.0000\n",
      "Epoch 144/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 1993450819944448.0000 - mae: 39418064.0000 - val_loss: 1963355581448192.0000 - val_mae: 39358004.0000\n",
      "Epoch 145/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1981030445613056.0000 - mae: 39322064.0000 - val_loss: 1994402155200512.0000 - val_mae: 40051784.0000\n",
      "Epoch 146/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1985459362201600.0000 - mae: 39421604.0000 - val_loss: 1968399483666432.0000 - val_mae: 39547692.0000\n",
      "Epoch 147/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 1978587817181184.0000 - mae: 39324212.0000 - val_loss: 1965787740897280.0000 - val_mae: 39481156.0000\n",
      "Epoch 148/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 1933065056157696.0000 - mae: 38838344.0000 - val_loss: 1960046040711168.0000 - val_mae: 39195632.0000\n",
      "Epoch 149/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1972439303061504.0000 - mae: 39169268.0000 - val_loss: 1966955569348608.0000 - val_mae: 39528944.0000\n",
      "Epoch 150/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 1961684168081408.0000 - mae: 39117384.0000 - val_loss: 1970603607195648.0000 - val_mae: 39623624.0000\n",
      "Epoch 151/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 1976498718244864.0000 - mae: 39307632.0000 - val_loss: 1969030441205760.0000 - val_mae: 39592556.0000\n",
      "Epoch 152/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1955024284418048.0000 - mae: 39014912.0000 - val_loss: 1958206586748928.0000 - val_mae: 39176488.0000\n",
      "Epoch 153/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1949546187849728.0000 - mae: 38948144.0000 - val_loss: 1959022630535168.0000 - val_mae: 39294848.0000\n",
      "Epoch 154/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 1973003957043200.0000 - mae: 39193268.0000 - val_loss: 1961083677966336.0000 - val_mae: 39396376.0000\n",
      "Epoch 155/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1960871748173824.0000 - mae: 39048052.0000 - val_loss: 1957028155097088.0000 - val_mae: 39084920.0000\n",
      "Epoch 156/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 1950695225819136.0000 - mae: 38955140.0000 - val_loss: 1956815151562752.0000 - val_mae: 39055596.0000\n",
      "Epoch 157/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 1973071200124928.0000 - mae: 39172468.0000 - val_loss: 1956478265065472.0000 - val_mae: 39237172.0000\n",
      "Epoch 158/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1958956326977536.0000 - mae: 39065180.0000 - val_loss: 1955450023051264.0000 - val_mae: 39186532.0000\n",
      "Epoch 159/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 1971072027066368.0000 - mae: 39251368.0000 - val_loss: 1958569243049984.0000 - val_mae: 39367504.0000\n",
      "Epoch 160/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 1976498584027136.0000 - mae: 39323572.0000 - val_loss: 1961369293291520.0000 - val_mae: 39460580.0000\n",
      "Epoch 161/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1948719138209792.0000 - mae: 38944364.0000 - val_loss: 1955959110893568.0000 - val_mae: 39296944.0000\n",
      "Epoch 162/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 960us/step - loss: 1963291425374208.0000 - mae: 39278916.0000 - val_loss: 1953182548754432.0000 - val_mae: 39147088.0000\n",
      "Epoch 163/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 988us/step - loss: 1996426426974208.0000 - mae: 39481736.0000 - val_loss: 1958260005404672.0000 - val_mae: 39400212.0000\n",
      "Epoch 164/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1924740738449408.0000 - mae: 38638584.0000 - val_loss: 1953436086042624.0000 - val_mae: 39238660.0000\n",
      "Epoch 165/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1951548045262848.0000 - mae: 38900508.0000 - val_loss: 1957385308471296.0000 - val_mae: 39394860.0000\n",
      "Epoch 166/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1955843012558848.0000 - mae: 39028056.0000 - val_loss: 1950700997181440.0000 - val_mae: 39083140.0000\n",
      "Epoch 167/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1937289961799680.0000 - mae: 38806816.0000 - val_loss: 1950141577691136.0000 - val_mae: 39041224.0000\n",
      "Epoch 168/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 1940993968439296.0000 - mae: 38849388.0000 - val_loss: 1953768409137152.0000 - val_mae: 39318396.0000\n",
      "Epoch 169/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 1945976533155840.0000 - mae: 38907896.0000 - val_loss: 1949337613500416.0000 - val_mae: 38974260.0000\n",
      "Epoch 170/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1931267612344320.0000 - mae: 38617448.0000 - val_loss: 1951046742048768.0000 - val_mae: 38867984.0000\n",
      "Epoch 171/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 1947661636730880.0000 - mae: 38863904.0000 - val_loss: 1947555336290304.0000 - val_mae: 39054388.0000\n",
      "Epoch 172/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1975099364212736.0000 - mae: 39251184.0000 - val_loss: 1956786428968960.0000 - val_mae: 39441816.0000\n",
      "Epoch 173/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1954037918334976.0000 - mae: 38931940.0000 - val_loss: 1950649726009344.0000 - val_mae: 39285244.0000\n",
      "Epoch 174/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1951435168153600.0000 - mae: 39089360.0000 - val_loss: 1948334201765888.0000 - val_mae: 39220404.0000\n",
      "Epoch 175/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1977736876785664.0000 - mae: 39239468.0000 - val_loss: 1945675751227392.0000 - val_mae: 39118320.0000\n",
      "Epoch 176/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1932503354966016.0000 - mae: 38664116.0000 - val_loss: 1946827876204544.0000 - val_mae: 39201424.0000\n",
      "Epoch 177/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1964395768840192.0000 - mae: 39131800.0000 - val_loss: 1947428768972800.0000 - val_mae: 39241228.0000\n",
      "Epoch 178/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1941825581481984.0000 - mae: 38833972.0000 - val_loss: 1943900453339136.0000 - val_mae: 39121248.0000\n",
      "Epoch 179/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1953635265150976.0000 - mae: 38988304.0000 - val_loss: 1942540156665856.0000 - val_mae: 38861044.0000\n",
      "Epoch 180/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1949469549527040.0000 - mae: 38989216.0000 - val_loss: 1941014772187136.0000 - val_mae: 38901344.0000\n",
      "Epoch 181/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1925662680023040.0000 - mae: 38578492.0000 - val_loss: 1940635338670080.0000 - val_mae: 39043932.0000\n",
      "Epoch 182/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1953931617894400.0000 - mae: 39002620.0000 - val_loss: 1948739807739904.0000 - val_mae: 39342924.0000\n",
      "Epoch 183/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1930400968474624.0000 - mae: 38691396.0000 - val_loss: 1938171772272640.0000 - val_mae: 38931968.0000\n",
      "Epoch 184/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1941176236113920.0000 - mae: 38735368.0000 - val_loss: 1946188731383808.0000 - val_mae: 39303608.0000\n",
      "Epoch 185/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1962677647704064.0000 - mae: 39161420.0000 - val_loss: 1937317476433920.0000 - val_mae: 39023644.0000\n",
      "Epoch 186/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1921113772785664.0000 - mae: 38617012.0000 - val_loss: 1935757463781376.0000 - val_mae: 38970920.0000\n",
      "Epoch 187/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1960735785615360.0000 - mae: 39113888.0000 - val_loss: 1935651431776256.0000 - val_mae: 38750948.0000\n",
      "Epoch 188/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1948427885740032.0000 - mae: 38913252.0000 - val_loss: 1942374129336320.0000 - val_mae: 39260776.0000\n",
      "Epoch 189/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1915669499084800.0000 - mae: 38558180.0000 - val_loss: 1932322161033216.0000 - val_mae: 38895720.0000\n",
      "Epoch 190/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1929743167389696.0000 - mae: 38714252.0000 - val_loss: 1933560453791744.0000 - val_mae: 39033168.0000\n",
      "Epoch 191/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1926147071803392.0000 - mae: 38553360.0000 - val_loss: 1935138451619840.0000 - val_mae: 39114484.0000\n",
      "Epoch 192/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1926501540823040.0000 - mae: 38669208.0000 - val_loss: 1929195693277184.0000 - val_mae: 38736724.0000\n",
      "Epoch 193/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1928586479009792.0000 - mae: 38687072.0000 - val_loss: 1940984573198336.0000 - val_mae: 39292704.0000\n",
      "Epoch 194/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1914345709633536.0000 - mae: 38515568.0000 - val_loss: 1926791853768704.0000 - val_mae: 38869884.0000\n",
      "Epoch 195/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1934522526466048.0000 - mae: 38694484.0000 - val_loss: 1926698304012288.0000 - val_mae: 38929040.0000\n",
      "Epoch 196/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1939042174238720.0000 - mae: 38896784.0000 - val_loss: 1923665117577216.0000 - val_mae: 38792028.0000\n",
      "Epoch 197/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1924612828954624.0000 - mae: 38538540.0000 - val_loss: 1923352792924160.0000 - val_mae: 38868752.0000\n",
      "Epoch 198/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1925907090505728.0000 - mae: 38657124.0000 - val_loss: 1922042022592512.0000 - val_mae: 38588904.0000\n",
      "Epoch 199/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1948750142504960.0000 - mae: 38986904.0000 - val_loss: 1919631069544448.0000 - val_mae: 38786164.0000\n",
      "Epoch 200/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1934439982563328.0000 - mae: 38631364.0000 - val_loss: 1925809782652928.0000 - val_mae: 39056284.0000\n",
      "Epoch 201/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1913656635817984.0000 - mae: 38571788.0000 - val_loss: 1920526301790208.0000 - val_mae: 38439188.0000\n",
      "Epoch 202/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1937302309830656.0000 - mae: 38763256.0000 - val_loss: 1917114889797632.0000 - val_mae: 38857196.0000\n",
      "Epoch 203/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1922374882557952.0000 - mae: 38650252.0000 - val_loss: 1917506402910208.0000 - val_mae: 38909720.0000\n",
      "Epoch 204/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1924300101648384.0000 - mae: 38708648.0000 - val_loss: 1923541503049728.0000 - val_mae: 39086996.0000\n",
      "Epoch 205/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1902540589367296.0000 - mae: 38404724.0000 - val_loss: 1908360941142016.0000 - val_mae: 38568456.0000\n",
      "Epoch 206/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1906637317079040.0000 - mae: 38400248.0000 - val_loss: 1907492552441856.0000 - val_mae: 38430888.0000\n",
      "Epoch 207/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1902858819600384.0000 - mae: 38325372.0000 - val_loss: 1905971194494976.0000 - val_mae: 38690468.0000\n",
      "Epoch 208/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1911500293799936.0000 - mae: 38480944.0000 - val_loss: 1903513802113024.0000 - val_mae: 38655236.0000\n",
      "Epoch 209/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1908928279478272.0000 - mae: 38410788.0000 - val_loss: 1900012330024960.0000 - val_mae: 38556044.0000\n",
      "Epoch 210/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1875331837329408.0000 - mae: 37971404.0000 - val_loss: 1899282722455552.0000 - val_mae: 38625644.0000\n",
      "Epoch 211/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1906296135614464.0000 - mae: 38506448.0000 - val_loss: 1895544490295296.0000 - val_mae: 38298524.0000\n",
      "Epoch 212/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 988us/step - loss: 1886120392523776.0000 - mae: 38165604.0000 - val_loss: 1901400812421120.0000 - val_mae: 38788064.0000\n",
      "Epoch 213/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1915833513148416.0000 - mae: 38557208.0000 - val_loss: 1889489391714304.0000 - val_mae: 38276860.0000\n",
      "Epoch 214/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1896197727977472.0000 - mae: 38255624.0000 - val_loss: 1894217479618560.0000 - val_mae: 38684436.0000\n",
      "Epoch 215/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 1889477177901056.0000 - mae: 38199808.0000 - val_loss: 1885514533699584.0000 - val_mae: 38471024.0000\n",
      "Epoch 216/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 1877405098573824.0000 - mae: 38116292.0000 - val_loss: 1889764806492160.0000 - val_mae: 38657232.0000\n",
      "Epoch 217/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1877881303072768.0000 - mae: 38100508.0000 - val_loss: 1881192588640256.0000 - val_mae: 38475760.0000\n",
      "Epoch 218/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1881546923442176.0000 - mae: 38192200.0000 - val_loss: 1878219397529600.0000 - val_mae: 38451548.0000\n",
      "Epoch 219/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1885751964860416.0000 - mae: 38190732.0000 - val_loss: 1871444757708800.0000 - val_mae: 37954300.0000\n",
      "Epoch 220/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1873413329125376.0000 - mae: 37854244.0000 - val_loss: 1870832322215936.0000 - val_mae: 38366668.0000\n",
      "Epoch 221/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1866697476669440.0000 - mae: 38001816.0000 - val_loss: 1892026240991232.0000 - val_mae: 38844668.0000\n",
      "Epoch 222/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 1860008870412288.0000 - mae: 37922280.0000 - val_loss: 1857334313746432.0000 - val_mae: 37902200.0000\n",
      "Epoch 223/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1878044377612288.0000 - mae: 38042016.0000 - val_loss: 1852362083794944.0000 - val_mae: 37911904.0000\n",
      "Epoch 224/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1873966843035648.0000 - mae: 38055888.0000 - val_loss: 1864989153427456.0000 - val_mae: 38440120.0000\n",
      "Epoch 225/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1844915013156864.0000 - mae: 37813728.0000 - val_loss: 1861699476914176.0000 - val_mae: 38415608.0000\n",
      "Epoch 226/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 1860507086618624.0000 - mae: 37921904.0000 - val_loss: 1849658133446656.0000 - val_mae: 38212956.0000\n",
      "Epoch 227/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 1838306803318784.0000 - mae: 37640900.0000 - val_loss: 1831932534980608.0000 - val_mae: 37750300.0000\n",
      "Epoch 228/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 1851966678368256.0000 - mae: 37783464.0000 - val_loss: 1827940631314432.0000 - val_mae: 37425128.0000\n",
      "Epoch 229/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1823630900068352.0000 - mae: 37357296.0000 - val_loss: 1828323554492416.0000 - val_mae: 37917004.0000\n",
      "Epoch 230/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 986us/step - loss: 1817620495990784.0000 - mae: 37417124.0000 - val_loss: 1812716582862848.0000 - val_mae: 37389196.0000\n",
      "Epoch 231/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1812806374522880.0000 - mae: 37322012.0000 - val_loss: 1805499695628288.0000 - val_mae: 37376440.0000\n",
      "Epoch 232/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1793605320572928.0000 - mae: 37041224.0000 - val_loss: 1801252107190272.0000 - val_mae: 37491176.0000\n",
      "Epoch 233/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 1807884207783936.0000 - mae: 37312440.0000 - val_loss: 1794497868464128.0000 - val_mae: 37441904.0000\n",
      "Epoch 234/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999us/step - loss: 1812443449786368.0000 - mae: 37335520.0000 - val_loss: 1786173416538112.0000 - val_mae: 37353976.0000\n",
      "Epoch 235/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1777083655127040.0000 - mae: 36814328.0000 - val_loss: 1772085118500864.0000 - val_mae: 37052092.0000\n",
      "Epoch 236/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1774489092227072.0000 - mae: 36725588.0000 - val_loss: 1760959408373760.0000 - val_mae: 36846584.0000\n",
      "Epoch 237/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1747282353455104.0000 - mae: 36445060.0000 - val_loss: 1752487048511488.0000 - val_mae: 36879840.0000\n",
      "Epoch 238/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1763961858949120.0000 - mae: 36719388.0000 - val_loss: 1738476462538752.0000 - val_mae: 36473304.0000\n",
      "Epoch 239/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 1759738832355328.0000 - mae: 36580864.0000 - val_loss: 1725562569621504.0000 - val_mae: 36439872.0000\n",
      "Epoch 240/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1726926355955712.0000 - mae: 36250748.0000 - val_loss: 1712522780475392.0000 - val_mae: 36343424.0000\n",
      "Epoch 241/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 1698558499618816.0000 - mae: 35868876.0000 - val_loss: 1701818379796480.0000 - val_mae: 36339336.0000\n",
      "Epoch 242/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1687397490229248.0000 - mae: 35763844.0000 - val_loss: 1702126006829056.0000 - val_mae: 36531108.0000\n",
      "Epoch 243/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1713418281156608.0000 - mae: 36069404.0000 - val_loss: 1662334342791168.0000 - val_mae: 35565820.0000\n",
      "Epoch 244/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1690801654464512.0000 - mae: 35763488.0000 - val_loss: 1644737995997184.0000 - val_mae: 35199324.0000\n",
      "Epoch 245/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1630770628132864.0000 - mae: 35092588.0000 - val_loss: 1627660803375104.0000 - val_mae: 35453264.0000\n",
      "Epoch 246/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1627799450288128.0000 - mae: 35046816.0000 - val_loss: 1598384024584192.0000 - val_mae: 34689152.0000\n",
      "Epoch 247/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - loss: 1610871776215040.0000 - mae: 34815008.0000 - val_loss: 1572309546565632.0000 - val_mae: 34577228.0000\n",
      "Epoch 248/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1569155966828544.0000 - mae: 34284524.0000 - val_loss: 1584826020790272.0000 - val_mae: 35189204.0000\n",
      "Epoch 249/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1559690395779072.0000 - mae: 34196432.0000 - val_loss: 1527009050099712.0000 - val_mae: 34306532.0000\n",
      "Epoch 250/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 986us/step - loss: 1508388085170176.0000 - mae: 33610748.0000 - val_loss: 1477114280804352.0000 - val_mae: 33396398.0000\n",
      "Epoch 251/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1495974287507456.0000 - mae: 33265258.0000 - val_loss: 1437265976885248.0000 - val_mae: 32709512.0000\n",
      "Epoch 252/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1450550042296320.0000 - mae: 32729566.0000 - val_loss: 1398293443641344.0000 - val_mae: 32429322.0000\n",
      "Epoch 253/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1381508577230848.0000 - mae: 31713232.0000 - val_loss: 1349007083307008.0000 - val_mae: 31595702.0000\n",
      "Epoch 254/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1345103360688128.0000 - mae: 31308636.0000 - val_loss: 1303864158453760.0000 - val_mae: 31229620.0000\n",
      "Epoch 255/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 1300818489769984.0000 - mae: 30774578.0000 - val_loss: 1257885862461440.0000 - val_mae: 30732902.0000\n",
      "Epoch 256/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1252910277066752.0000 - mae: 30155074.0000 - val_loss: 1187863802675200.0000 - val_mae: 28863136.0000\n",
      "Epoch 257/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1212256129908736.0000 - mae: 29434450.0000 - val_loss: 1114050628943872.0000 - val_mae: 28144304.0000\n",
      "Epoch 258/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 1109978328858624.0000 - mae: 27918116.0000 - val_loss: 1053964506234880.0000 - val_mae: 27730728.0000\n",
      "Epoch 259/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 1047803442757632.0000 - mae: 27083050.0000 - val_loss: 973489032921088.0000 - val_mae: 25556226.0000\n",
      "Epoch 260/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 952726557032448.0000 - mae: 25592722.0000 - val_loss: 956782281228288.0000 - val_mae: 23842772.0000\n",
      "Epoch 261/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 882606585413632.0000 - mae: 24130220.0000 - val_loss: 803355714650112.0000 - val_mae: 22802774.0000\n",
      "Epoch 262/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 813581696237568.0000 - mae: 23083222.0000 - val_loss: 732471775723520.0000 - val_mae: 21081794.0000\n",
      "Epoch 263/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 720236420530176.0000 - mae: 21377204.0000 - val_loss: 656391463239680.0000 - val_mae: 20965432.0000\n",
      "Epoch 264/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 638125571309568.0000 - mae: 19798072.0000 - val_loss: 561145429622784.0000 - val_mae: 18859704.0000\n",
      "Epoch 265/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 562911265161216.0000 - mae: 18391124.0000 - val_loss: 495701100331008.0000 - val_mae: 17610308.0000\n",
      "Epoch 266/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 482688121176064.0000 - mae: 16601408.0000 - val_loss: 416935124664320.0000 - val_mae: 14856880.0000\n",
      "Epoch 267/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 419708398469120.0000 - mae: 15140173.0000 - val_loss: 360051269173248.0000 - val_mae: 14521374.0000\n",
      "Epoch 268/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 358737042735104.0000 - mae: 13933663.0000 - val_loss: 309739418288128.0000 - val_mae: 11919955.0000\n",
      "Epoch 269/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 306308041408512.0000 - mae: 12474730.0000 - val_loss: 245170540904448.0000 - val_mae: 11297178.0000\n",
      "Epoch 270/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 252219337211904.0000 - mae: 11097649.0000 - val_loss: 206284930416640.0000 - val_mae: 10311752.0000\n",
      "Epoch 271/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 219807668502528.0000 - mae: 10164588.0000 - val_loss: 184335583936512.0000 - val_mae: 9832608.0000\n",
      "Epoch 272/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 176781709541376.0000 - mae: 8943233.0000 - val_loss: 139270471811072.0000 - val_mae: 7654940.0000\n",
      "Epoch 273/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 149446809616384.0000 - mae: 8014942.5000 - val_loss: 124637635674112.0000 - val_mae: 7814478.0000\n",
      "Epoch 274/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 120069711462400.0000 - mae: 7142141.0000 - val_loss: 132921973276672.0000 - val_mae: 8226743.5000\n",
      "Epoch 275/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 108641357135872.0000 - mae: 6557769.0000 - val_loss: 113912565465088.0000 - val_mae: 7515660.5000\n",
      "Epoch 276/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 88189343629312.0000 - mae: 5844475.0000 - val_loss: 71882745315328.0000 - val_mae: 4731982.5000\n",
      "Epoch 277/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69493309046784.0000 - mae: 5115059.5000 - val_loss: 82881007321088.0000 - val_mae: 6269631.5000\n",
      "Epoch 278/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974us/step - loss: 66225325473792.0000 - mae: 4928852.0000 - val_loss: 43917911261184.0000 - val_mae: 4177078.2500\n",
      "Epoch 279/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 46086571950080.0000 - mae: 4064979.5000 - val_loss: 34081031061504.0000 - val_mae: 3538116.5000\n",
      "Epoch 280/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 40941884801024.0000 - mae: 3791384.2500 - val_loss: 32211415859200.0000 - val_mae: 3682105.0000\n",
      "Epoch 281/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 34318254604288.0000 - mae: 3414764.7500 - val_loss: 68800833650688.0000 - val_mae: 5734039.0000\n",
      "Epoch 282/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 29633483374592.0000 - mae: 3156360.7500 - val_loss: 19391766855680.0000 - val_mae: 2347124.0000\n",
      "Epoch 283/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 26381742243840.0000 - mae: 2945719.2500 - val_loss: 26518916956160.0000 - val_mae: 2482815.2500\n",
      "Epoch 284/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 22425362235392.0000 - mae: 2723233.7500 - val_loss: 12500431011840.0000 - val_mae: 1972418.1250\n",
      "Epoch 285/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 17898120675328.0000 - mae: 2403180.5000 - val_loss: 10832248307712.0000 - val_mae: 1908171.1250\n",
      "Epoch 286/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18016515391488.0000 - mae: 2394027.7500 - val_loss: 15314977816576.0000 - val_mae: 2638660.7500\n",
      "Epoch 287/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13407825690624.0000 - mae: 2070793.8750 - val_loss: 8699984740352.0000 - val_mae: 1508429.2500\n",
      "Epoch 288/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 10298604912640.0000 - mae: 1793300.5000 - val_loss: 6527799787520.0000 - val_mae: 1511933.1250\n",
      "Epoch 289/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 12530748489728.0000 - mae: 2004131.7500 - val_loss: 5507813212160.0000 - val_mae: 1155954.1250\n",
      "Epoch 290/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8642434170880.0000 - mae: 1643718.0000 - val_loss: 5054492835840.0000 - val_mae: 1276875.7500\n",
      "Epoch 291/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 7643874721792.0000 - mae: 1576815.6250 - val_loss: 3981756596224.0000 - val_mae: 1084608.7500\n",
      "Epoch 292/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 10520588451840.0000 - mae: 1793111.1250 - val_loss: 7228556836864.0000 - val_mae: 1227186.5000\n",
      "Epoch 293/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 5677398884352.0000 - mae: 1335680.2500 - val_loss: 7394217164800.0000 - val_mae: 2044714.6250\n",
      "Epoch 294/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6993558896640.0000 - mae: 1497867.7500 - val_loss: 3484237955072.0000 - val_mae: 1047273.7500\n",
      "Epoch 295/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 984us/step - loss: 5056335183872.0000 - mae: 1250793.5000 - val_loss: 2887773847552.0000 - val_mae: 745873.7500\n",
      "Epoch 296/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 6724588142592.0000 - mae: 1419650.3750 - val_loss: 10264298651648.0000 - val_mae: 2113434.0000\n",
      "Epoch 297/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4533021310976.0000 - mae: 1208749.1250 - val_loss: 5701276008448.0000 - val_mae: 1614632.7500\n",
      "Epoch 298/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3787514707968.0000 - mae: 1089505.0000 - val_loss: 2365117169664.0000 - val_mae: 787898.1875\n",
      "Epoch 299/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 4448719470592.0000 - mae: 1174985.5000 - val_loss: 1655113121792.0000 - val_mae: 609453.4375\n",
      "Epoch 300/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4515467100160.0000 - mae: 1220358.6250 - val_loss: 30293513732096.0000 - val_mae: 4033908.2500\n",
      "Epoch 301/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 5031074463744.0000 - mae: 1271061.3750 - val_loss: 10131697827840.0000 - val_mae: 2068426.1250\n",
      "Epoch 302/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3783350288384.0000 - mae: 1162513.2500 - val_loss: 2002772033536.0000 - val_mae: 802427.0625\n",
      "Epoch 303/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2504131870720.0000 - mae: 906246.0000 - val_loss: 1394041421824.0000 - val_mae: 853780.7500\n",
      "Epoch 304/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2096495984640.0000 - mae: 827744.1875 - val_loss: 11698098405376.0000 - val_mae: 1772347.3750\n",
      "Epoch 305/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 14064454467584.0000 - mae: 1926527.0000 - val_loss: 4469649047552.0000 - val_mae: 1251153.6250\n",
      "Epoch 306/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 3426680832000.0000 - mae: 1059041.0000 - val_loss: 1277569400832.0000 - val_mae: 656847.1250\n",
      "Epoch 307/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 2271801245696.0000 - mae: 878378.9375 - val_loss: 2029761200128.0000 - val_mae: 889259.6250\n",
      "Epoch 308/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3935335350272.0000 - mae: 1072850.8750 - val_loss: 5854880858112.0000 - val_mae: 991978.0625\n",
      "Epoch 309/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8970328604672.0000 - mae: 1667237.2500 - val_loss: 8776403910656.0000 - val_mae: 1500225.1250\n",
      "Epoch 310/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 11539753992192.0000 - mae: 1803684.0000 - val_loss: 6180447977472.0000 - val_mae: 1319428.2500\n",
      "Epoch 311/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 7531331584000.0000 - mae: 1542732.8750 - val_loss: 2823891976192.0000 - val_mae: 830077.4375\n",
      "Epoch 312/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6311649476608.0000 - mae: 1393468.8750 - val_loss: 2695036928000.0000 - val_mae: 895672.0000\n",
      "Epoch 313/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5470148886528.0000 - mae: 1256572.0000 - val_loss: 2353097605120.0000 - val_mae: 867596.8125\n",
      "Epoch 313: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step\n",
      "Metrics:\n",
      "Accuracy: 0.0\n",
      "MSE: 2353097424719.122\n",
      "MAE: 867596.7760021973\n",
      "RMSE: 1533980.9075471319\n",
      "F1 Score: 0.0\n",
      "Training Time: 166.03598999977112\n",
      "Inference Time: 0.13128447532653809\n",
      "Model type: RNN\n",
      "dataset: multiplication_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"multiplication_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=1500,ea=True,eamonitor=\"mae\")\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972ms/step - loss: 546036578368094208.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 546036681447309312.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036681447309312.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036681447309312.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036681447309312.0000 - mae: 516242784.0000 - val_loss: 148205663388958720.0000 - val_mae: 192491344.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205663388958720.0000 - val_mae: 192491344.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205663388958720.0000 - val_mae: 192491344.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Metrics:\n",
      "Accuracy: 0.0\n",
      "MSE: 1.4820564701812893e+17\n",
      "MAE: 192491336.80109167\n",
      "RMSE: 384974865.4368606\n",
      "F1 Score: 0.0\n",
      "Training Time: 3.2142186164855957\n",
      "Inference Time: 0.04163074493408203\n",
      "Model type: RNN\n",
      "dataset: exponent_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"exponent_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv',nrows=16)\n",
    "d_set['result']=d_set['result'].astype('int')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=50)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 103303.7500 - mae: 106.4455 - val_loss: 13.9249 - val_mae: 3.2059\n",
      "Epoch 2/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.0681 - mae: 3.2254 - val_loss: 13.5540 - val_mae: 3.1882\n",
      "Epoch 3/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.5988 - mae: 3.1694 - val_loss: 12.8779 - val_mae: 3.0972\n",
      "Epoch 4/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.2397 - mae: 3.1309 - val_loss: 12.4179 - val_mae: 2.9951\n",
      "Epoch 5/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.8028 - mae: 3.0843 - val_loss: 12.1711 - val_mae: 2.9490\n",
      "Epoch 6/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.7686 - mae: 3.0517 - val_loss: 10.0734 - val_mae: 2.7654\n",
      "Epoch 7/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 10.9437 - mae: 2.8558 - val_loss: 8.6625 - val_mae: 2.5377\n",
      "Epoch 8/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 10.2934 - mae: 2.7880 - val_loss: 8.2727 - val_mae: 2.4426\n",
      "Epoch 9/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 10.7794 - mae: 2.8117 - val_loss: 6.1318 - val_mae: 2.1150\n",
      "Epoch 10/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9.7375 - mae: 2.6674 - val_loss: 34.8240 - val_mae: 5.8053\n",
      "Epoch 11/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.8151 - mae: 2.7679 - val_loss: 78.9054 - val_mae: 8.4183\n",
      "Epoch 12/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 47.1062 - mae: 4.2638 - val_loss: 7.3846 - val_mae: 2.6736\n",
      "Epoch 13/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 38.2583 - mae: 3.7653 - val_loss: 2.5898 - val_mae: 1.3938\n",
      "Epoch 14/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 77.9381 - mae: 3.5561 - val_loss: 1.5971 - val_mae: 1.1179\n",
      "Epoch 15/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.7214 - mae: 1.5043 - val_loss: 67.0397 - val_mae: 7.5289\n",
      "Epoch 16/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9.4579 - mae: 2.1179 - val_loss: 1.1601 - val_mae: 0.9420\n",
      "Epoch 17/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.6307 - mae: 1.5744 - val_loss: 0.5939 - val_mae: 0.6793\n",
      "Epoch 18/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27.0483 - mae: 3.2273 - val_loss: 23.4709 - val_mae: 4.4599\n",
      "Epoch 19/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18.3712 - mae: 2.4336 - val_loss: 0.6239 - val_mae: 0.6671\n",
      "Epoch 20/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.4304 - mae: 1.3985 - val_loss: 0.5370 - val_mae: 0.6506\n",
      "Epoch 21/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 19.6306 - mae: 1.9755 - val_loss: 2.0167 - val_mae: 1.0771\n",
      "Epoch 22/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9162 - mae: 0.7785 - val_loss: 0.7928 - val_mae: 0.8379\n",
      "Epoch 23/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6.7712 - mae: 1.4803 - val_loss: 2.2823 - val_mae: 1.4590\n",
      "Epoch 24/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6280 - mae: 0.6484 - val_loss: 1.0429 - val_mae: 0.9798\n",
      "Epoch 25/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.3237 - mae: 1.3065 - val_loss: 0.2130 - val_mae: 0.4061\n",
      "Epoch 26/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1803 - mae: 0.7438 - val_loss: 0.4451 - val_mae: 0.5161\n",
      "Epoch 27/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 16.7184 - mae: 2.1368 - val_loss: 0.3602 - val_mae: 0.5316\n",
      "Epoch 28/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8071 - mae: 0.7370 - val_loss: 0.5262 - val_mae: 0.6670\n",
      "Epoch 29/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1274 - mae: 0.7656 - val_loss: 0.4376 - val_mae: 0.5209\n",
      "Epoch 30/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.7281 - mae: 1.2082 - val_loss: 4.4803 - val_mae: 1.9744\n",
      "Epoch 31/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 15.8219 - mae: 2.4844 - val_loss: 0.2862 - val_mae: 0.4738\n",
      "Epoch 31: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step\n",
      "Metrics:\n",
      "Accuracy: 0.47125\n",
      "MSE: 0.28615084854360623\n",
      "MAE: 0.47383598344014505\n",
      "RMSE: 0.5349306950845186\n",
      "F1 Score: 0.51475\n",
      "Training Time: 19.291871070861816\n",
      "Inference Time: 0.1859419345855713\n",
      "Model type: RNN\n",
      "dataset: natural_log_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"natural_log_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=1500,ea=True,eamonitor=\"mae\")\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1097.3455 - mae: 22.4542 - val_loss: 980.0704 - val_mae: 21.8617\n",
      "Epoch 2/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 808.8227 - mae: 19.2860 - val_loss: 49.3797 - val_mae: 4.4705\n",
      "Epoch 3/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 31.2329 - mae: 3.1581 - val_loss: 24.9691 - val_mae: 2.4206\n",
      "Epoch 4/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23.3472 - mae: 2.3172 - val_loss: 22.1369 - val_mae: 2.1846\n",
      "Epoch 5/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23.0436 - mae: 2.2373 - val_loss: 20.3808 - val_mae: 2.2172\n",
      "Epoch 6/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24.3902 - mae: 2.3762 - val_loss: 24.0295 - val_mae: 2.2230\n",
      "Epoch 7/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23.0300 - mae: 2.2455 - val_loss: 19.8614 - val_mae: 2.1552\n",
      "Epoch 8/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 22.6940 - mae: 2.2476 - val_loss: 19.9909 - val_mae: 2.0615\n",
      "Epoch 9/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23.2227 - mae: 2.2807 - val_loss: 20.1729 - val_mae: 2.1313\n",
      "Epoch 10/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18.2457 - mae: 2.0447 - val_loss: 9.2089 - val_mae: 1.4494\n",
      "Epoch 11/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6.4126 - mae: 1.2144 - val_loss: 5.6015 - val_mae: 1.1521\n",
      "Epoch 12/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.8131 - mae: 1.0942 - val_loss: 4.8318 - val_mae: 0.9534\n",
      "Epoch 13/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.5394 - mae: 1.0527 - val_loss: 5.9643 - val_mae: 1.2819\n",
      "Epoch 14/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.9055 - mae: 0.9596 - val_loss: 3.9801 - val_mae: 0.6910\n",
      "Epoch 15/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.5350 - mae: 0.8790 - val_loss: 3.5159 - val_mae: 0.6900\n",
      "Epoch 16/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.6865 - mae: 0.7822 - val_loss: 2.8747 - val_mae: 0.6236\n",
      "Epoch 17/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.2221 - mae: 0.7255 - val_loss: 2.7094 - val_mae: 0.5905\n",
      "Epoch 18/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.0519 - mae: 0.7309 - val_loss: 2.5485 - val_mae: 0.7789\n",
      "Epoch 19/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.4079 - mae: 0.6397 - val_loss: 2.0395 - val_mae: 0.5843\n",
      "Epoch 20/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.4703 - mae: 0.7031 - val_loss: 1.9748 - val_mae: 0.5315\n",
      "Epoch 21/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9029 - mae: 0.5722 - val_loss: 2.5481 - val_mae: 0.8409\n",
      "Epoch 22/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8529 - mae: 0.5653 - val_loss: 1.4622 - val_mae: 0.4084\n",
      "Epoch 23/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.6996 - mae: 0.5149 - val_loss: 1.3640 - val_mae: 0.5172\n",
      "Epoch 24/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.7558 - mae: 0.5556 - val_loss: 1.3774 - val_mae: 0.5744\n",
      "Epoch 25/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3546 - mae: 0.4777 - val_loss: 1.1272 - val_mae: 0.4696\n",
      "Epoch 26/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.2618 - mae: 0.4662 - val_loss: 1.0120 - val_mae: 0.3849\n",
      "Epoch 27/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3507 - mae: 0.5239 - val_loss: 1.1046 - val_mae: 0.4493\n",
      "Epoch 28/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1957 - mae: 0.4885 - val_loss: 0.8545 - val_mae: 0.3894\n",
      "Epoch 29/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9355 - mae: 0.3673 - val_loss: 0.9555 - val_mae: 0.4899\n",
      "Epoch 30/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9744 - mae: 0.4156 - val_loss: 0.8173 - val_mae: 0.3831\n",
      "Epoch 31/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9302 - mae: 0.4219 - val_loss: 0.8216 - val_mae: 0.4481\n",
      "Epoch 32/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8581 - mae: 0.4219 - val_loss: 0.7993 - val_mae: 0.3642\n",
      "Epoch 33/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8476 - mae: 0.3846 - val_loss: 0.5419 - val_mae: 0.2703\n",
      "Epoch 34/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7219 - mae: 0.3505 - val_loss: 0.6824 - val_mae: 0.4598\n",
      "Epoch 35/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7014 - mae: 0.3942 - val_loss: 0.5277 - val_mae: 0.2865\n",
      "Epoch 36/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8596 - mae: 0.4523 - val_loss: 0.4384 - val_mae: 0.2690\n",
      "Epoch 37/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6241 - mae: 0.3588 - val_loss: 0.9536 - val_mae: 0.5626\n",
      "Epoch 38/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5756 - mae: 0.3801 - val_loss: 0.3784 - val_mae: 0.2672\n",
      "Epoch 39/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5653 - mae: 0.3468 - val_loss: 0.3733 - val_mae: 0.2776\n",
      "Epoch 40/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5130 - mae: 0.3488 - val_loss: 0.3284 - val_mae: 0.2184\n",
      "Epoch 41/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5131 - mae: 0.3764 - val_loss: 0.2911 - val_mae: 0.2217\n",
      "Epoch 42/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4773 - mae: 0.3284 - val_loss: 0.4899 - val_mae: 0.3897\n",
      "Epoch 43/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4307 - mae: 0.3334 - val_loss: 0.2447 - val_mae: 0.2040\n",
      "Epoch 44/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4244 - mae: 0.3266 - val_loss: 0.3038 - val_mae: 0.3001\n",
      "Epoch 45/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3544 - mae: 0.2942 - val_loss: 0.4147 - val_mae: 0.3500\n",
      "Epoch 46/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3004 - mae: 0.2669 - val_loss: 0.2570 - val_mae: 0.2569\n",
      "Epoch 47/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3516 - mae: 0.3053 - val_loss: 0.4187 - val_mae: 0.4310\n",
      "Epoch 48/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4699 - mae: 0.3729 - val_loss: 0.1899 - val_mae: 0.2216\n",
      "Epoch 49/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2746 - mae: 0.2633 - val_loss: 0.1920 - val_mae: 0.2109\n",
      "Epoch 50/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2780 - mae: 0.2812 - val_loss: 0.1660 - val_mae: 0.2392\n",
      "Epoch 51/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2714 - mae: 0.2689 - val_loss: 0.1794 - val_mae: 0.2080\n",
      "Epoch 52/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3509 - mae: 0.3097 - val_loss: 0.1208 - val_mae: 0.1456\n",
      "Epoch 53/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2189 - mae: 0.2458 - val_loss: 0.1345 - val_mae: 0.1754\n",
      "Epoch 53: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step\n",
      "Metrics:\n",
      "Accuracy: 0.91925\n",
      "MSE: 0.1344752128080965\n",
      "MAE: 0.17543116304724737\n",
      "RMSE: 0.3667086211259513\n",
      "F1 Score: 0.82175\n",
      "Training Time: 32.025848150253296\n",
      "Inference Time: 0.1475062370300293\n",
      "Model type: RNN\n",
      "dataset: integration_with_cos_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"integration_with_cos_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=1500,ea=True,eamonitor=\"mae\")\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 667.8182 - mae: 18.7644 - val_loss: 590.3171 - val_mae: 17.4336\n",
      "Epoch 2/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 551.2769 - mae: 16.1618 - val_loss: 425.7437 - val_mae: 11.7985\n",
      "Epoch 3/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 412.6239 - mae: 10.9820 - val_loss: 385.2934 - val_mae: 9.8459\n",
      "Epoch 4/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 379.0697 - mae: 9.6671 - val_loss: 331.5040 - val_mae: 8.7874\n",
      "Epoch 5/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 296.0459 - mae: 8.4165 - val_loss: 156.0409 - val_mae: 5.7509\n",
      "Epoch 6/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 111.7848 - mae: 4.8896 - val_loss: 19.4805 - val_mae: 1.8940\n",
      "Epoch 7/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 16.7178 - mae: 1.8250 - val_loss: 3.1313 - val_mae: 0.7589\n",
      "Epoch 8/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.8809 - mae: 0.8092 - val_loss: 0.6011 - val_mae: 0.3927\n",
      "Epoch 9/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7048 - mae: 0.4583 - val_loss: 0.1878 - val_mae: 0.2405\n",
      "Epoch 10/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2653 - mae: 0.3256 - val_loss: 0.1428 - val_mae: 0.2492\n",
      "Epoch 11/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1564 - mae: 0.2620 - val_loss: 0.0790 - val_mae: 0.1970\n",
      "Epoch 12/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1451 - mae: 0.2707 - val_loss: 0.0503 - val_mae: 0.1702\n",
      "Epoch 13/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0903 - mae: 0.2137 - val_loss: 2.4360 - val_mae: 0.8872\n",
      "Epoch 14/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4233 - mae: 0.4082 - val_loss: 0.0294 - val_mae: 0.1156\n",
      "Epoch 15/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2497 - mae: 0.3162 - val_loss: 0.1018 - val_mae: 0.2855\n",
      "Epoch 16/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2908 - mae: 0.3383 - val_loss: 0.1366 - val_mae: 0.3318\n",
      "Epoch 17/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4089 - mae: 0.3849 - val_loss: 0.4253 - val_mae: 0.4622\n",
      "Epoch 18/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4167 - mae: 0.4062 - val_loss: 0.0308 - val_mae: 0.1240\n",
      "Epoch 19/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1935 - mae: 0.2057 - val_loss: 0.4867 - val_mae: 0.4751\n",
      "Epoch 20/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0973 - mae: 0.2038 - val_loss: 0.0285 - val_mae: 0.1157\n",
      "Epoch 21/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0606 - mae: 0.1619 - val_loss: 0.0244 - val_mae: 0.1000\n",
      "Epoch 22/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0406 - mae: 0.1368 - val_loss: 0.0543 - val_mae: 0.1684\n",
      "Epoch 23/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4162 - mae: 0.3536 - val_loss: 0.0798 - val_mae: 0.2480\n",
      "Epoch 24/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1245 - mae: 0.2172 - val_loss: 0.3921 - val_mae: 0.4282\n",
      "Epoch 25/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2402 - mae: 0.2767 - val_loss: 0.7310 - val_mae: 0.7075\n",
      "Epoch 26/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1819 - mae: 0.2833 - val_loss: 0.0581 - val_mae: 0.1804\n",
      "Epoch 27/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1218 - mae: 0.1982 - val_loss: 0.0331 - val_mae: 0.1428\n",
      "Epoch 28/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1181 - mae: 0.2286 - val_loss: 0.0572 - val_mae: 0.1598\n",
      "Epoch 28: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "Metrics:\n",
      "Accuracy: 0.9345\n",
      "MSE: 0.05719731033128839\n",
      "MAE: 0.15984060072290326\n",
      "RMSE: 0.23915959176100043\n",
      "F1 Score: 0.85175\n",
      "Training Time: 18.01930856704712\n",
      "Inference Time: 0.13095307350158691\n",
      "Model type: RNN\n",
      "dataset: integration_with_sin_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"integration_with_sin_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=1500,ea=True,eamonitor=\"mae\")\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 39331124.0000 - mae: 199.1649 - val_loss: 11124388.0000 - val_mae: 211.5812\n",
      "Epoch 2/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 22051388.0000 - mae: 168.5208 - val_loss: 11124748.0000 - val_mae: 211.6656\n",
      "Epoch 3/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13317541.0000 - mae: 147.4144 - val_loss: 11125366.0000 - val_mae: 211.9076\n",
      "Epoch 4/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25502118.0000 - mae: 180.4657 - val_loss: 11126321.0000 - val_mae: 212.4852\n",
      "Epoch 5/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 56985080.0000 - mae: 228.0427 - val_loss: 11126742.0000 - val_mae: 212.7924\n",
      "Epoch 6/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 32967042.0000 - mae: 204.7207 - val_loss: 11120795.0000 - val_mae: 213.4782\n",
      "Epoch 7/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 78524424.0000 - mae: 278.7537 - val_loss: 11128220.0000 - val_mae: 214.1275\n",
      "Epoch 8/1500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 41541240.0000 - mae: 202.9734 - val_loss: 11132442.0000 - val_mae: 219.8622\n",
      "Epoch 8: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n",
      "Metrics:\n",
      "Accuracy: 0.00575\n",
      "MSE: 11132440.394581147\n",
      "MAE: 219.86224684493155\n",
      "RMSE: 3336.531191908918\n",
      "F1 Score: 0.0055\n",
      "Training Time: 5.551985740661621\n",
      "Inference Time: 0.11881470680236816\n",
      "Model type: RNN\n",
      "dataset: integration_with_tan_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"integration_with_tan_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(32, activation='relu', input_shape=(dset_features.shape[1],1)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')  # Output layer with linear activation for regression\n",
    "            ])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='RNN',dataset_name=dataset_name,total_epoch=1500,ea=True,eamonitor=\"mae\")\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='RNN',dataset_name=dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
