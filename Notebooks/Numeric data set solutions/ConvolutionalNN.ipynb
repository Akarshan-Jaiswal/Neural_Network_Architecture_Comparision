{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "\n",
    "# CBAM (Convolutional Block Attention Module) Layer\n",
    "class CBAM(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention()\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        channel_att = self.channel_attention(inputs)\n",
    "        spatial_att = self.spatial_attention(inputs)\n",
    "        return tf.multiply(inputs, tf.expand_dims(tf.expand_dims(channel_att, axis=1), axis=1)) * spatial_att\n",
    "\n",
    "# Channel Attention Layer\n",
    "class ChannelAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.global_avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        avg_pool = self.global_avgpool(inputs)\n",
    "        dense1_out = self.dense1(avg_pool)\n",
    "        attention = self.dense2(dense1_out)\n",
    "        return attention\n",
    "\n",
    "# Linear Attention Layer\n",
    "class LinearAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.dense = layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.sigmoid(self.dense(inputs))\n",
    "\n",
    "# NonLinear Attention Layer\n",
    "class NonLinearAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(NonLinearAttention, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense1_out = self.dense1(inputs)\n",
    "        attention = self.dense2(dense1_out)\n",
    "        return attention\n",
    "\n",
    "def configure_model(layers_config, optimizer='adam', loss='mean_squared_error', metrics=['accuracy'],debugflag=False):\n",
    "    if debugflag:\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Conv1D(32, kernel_size=2, activation='relu', input_shape=(2, 1)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        return model\n",
    "        \n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    for layer_config in layers_config:\n",
    "        layer_type = layer_config[0]\n",
    "\n",
    "        if layer_type == 'dense':\n",
    "            neurons, activation = layer_config[1], layer_config[2]\n",
    "            if len(layer_config) > 3 and layer_config[3] == 'input':\n",
    "                input_shape = layer_config[4]\n",
    "                model.add(layers.Dense(neurons, activation=activation, input_shape=(input_shape,)))\n",
    "            else:\n",
    "                model.add(layers.Dense(neurons, activation=activation))\n",
    "        elif layer_type == 'dropout':\n",
    "            rate = layer_config[1]\n",
    "            model.add(layers.Dropout(rate))\n",
    "        elif layer_type == 'activation':\n",
    "            activation = layer_config[1]\n",
    "            model.add(layers.Activation(activation))\n",
    "        elif layer_type == 'conv2d':\n",
    "            filters, kernel_size, strides, activation = layer_config[1], layer_config[2], layer_config[3], layer_config[4]\n",
    "            model.add(layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, activation=activation))\n",
    "        elif layer_type == 'maxpool2d':\n",
    "            pool_size = layer_config[1]\n",
    "            model.add(layers.MaxPool2D(pool_size=pool_size))\n",
    "        elif layer_type == 'rnn':\n",
    "            units, return_sequences = layer_config[1], layer_config[2]\n",
    "            model.add(layers.SimpleRNN(units, return_sequences=return_sequences))\n",
    "        elif layer_type == 'lstm':\n",
    "            units, return_sequences = layer_config[1], layer_config[2]\n",
    "            model.add(layers.LSTM(units, return_sequences=return_sequences))\n",
    "        elif layer_type == 'gru':\n",
    "            units, return_sequences = layer_config[1], layer_config[2]\n",
    "            model.add(layers.GRU(units, return_sequences=return_sequences))\n",
    "        elif layer_type == 'attention':\n",
    "            model.add(layers.MultiHeadAttention(num_heads=layer_config[1], key_dim=layer_config[2]))\n",
    "        elif layer_type == 'cbam':\n",
    "            model.add(CBAM())\n",
    "        elif layer_type == 'channel_attention':\n",
    "            model.add(ChannelAttention())\n",
    "        elif layer_type == 'linear_attention':\n",
    "            model.add(LinearAttention())\n",
    "        elif layer_type == 'nonlinear_attention':\n",
    "            model.add(NonLinearAttention())\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, model_name='perceptron', dataset_name=\"addition_dataset_2\", total_epoch=50,ea=False,eamonitor=\"val_loss\"):\n",
    "    tensorboard_callback = TensorBoard(log_dir='./../../Observation/'+model_name+'/'+model_name+dataset_name+'logs')\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor=eamonitor, patience=10, min_delta=0.001, verbose=1)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    if ea:\n",
    "        history = model.fit(X_train, y_train, epochs=total_epoch, validation_data=(X_test, y_test), callbacks=[tensorboard_callback,early_stopping])\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, epochs=total_epoch, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    return model, history, predictions, training_time\n",
    "\n",
    "def test_model(model, X_test, y_test, model_name='perceptron', dataset_name=\"addition_dataset_2\",threshold=0.5):\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    metrics = calculate_metrics(predictions, y_test,threshold)\n",
    "    metrics['Training Time'] = training_time\n",
    "    metrics['Inference Time'] = inference_time\n",
    "    metrics['Model type'] = model_name\n",
    "    metrics['dataset'] = dataset_name\n",
    "\n",
    "    print(\"Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    metrics_list = list(metrics.values())\n",
    "    pd.DataFrame([metrics_list], columns=metrics.keys()).to_csv('./../../Observation/'+model_name+'/'+model_name+dataset_name+'metrics.csv', index=False)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_metrics(predictions, true_labels, threshold=0.5):\n",
    "    #threshold = 0.5  # Define the threshold\n",
    "    interpredictions=predictions.flatten()\n",
    "    correct_predictions = np.sum(np.abs(true_labels - interpredictions) <= threshold)\n",
    "    total_predictions = len(true_labels)\n",
    "    accuracy = correct_predictions / total_predictions  # Final accuracy\n",
    "    mse = mean_squared_error(true_labels, interpredictions)  # Mean Squared Error\n",
    "    mae = mean_absolute_error(true_labels, interpredictions)  # Mean Absolute Error\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "    f1 = f1_score(true_labels.round(), interpredictions.round(), average='micro')  # F1 Score\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'F1 Score': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 94342992.0000 - mae: 5051.7275 - val_loss: 0.0600 - val_mae: 0.2041\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0618 - mae: 0.2072 - val_loss: 0.0600 - val_mae: 0.2054\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0635 - mae: 0.2083 - val_loss: 0.0598 - val_mae: 0.2038\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0611 - mae: 0.2059 - val_loss: 0.0596 - val_mae: 0.2037\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0610 - mae: 0.2057 - val_loss: 0.0595 - val_mae: 0.2022\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0610 - mae: 0.2058 - val_loss: 0.0601 - val_mae: 0.2018\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0603 - mae: 0.2041 - val_loss: 0.0597 - val_mae: 0.2075\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0592 - mae: 0.2023 - val_loss: 0.0586 - val_mae: 0.2047\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1144 - mae: 0.2198 - val_loss: 0.0578 - val_mae: 0.2034\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0592 - mae: 0.2021 - val_loss: 0.0570 - val_mae: 0.1969\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0583 - mae: 0.2015 - val_loss: 0.0566 - val_mae: 0.1958\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0569 - mae: 0.1986 - val_loss: 0.0540 - val_mae: 0.1927\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0558 - mae: 0.1969 - val_loss: 0.0532 - val_mae: 0.1961\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0555 - mae: 0.1972 - val_loss: 0.0516 - val_mae: 0.1863\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0536 - mae: 0.1942 - val_loss: 0.1409 - val_mae: 0.3149\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0625 - mae: 0.2121 - val_loss: 0.0435 - val_mae: 0.1755\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0575 - mae: 0.2005 - val_loss: 0.0521 - val_mae: 0.1969\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0625 - mae: 0.2134 - val_loss: 0.0487 - val_mae: 0.1909\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5700 - mae: 0.3832 - val_loss: 169.3091 - val_mae: 11.2121\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 315.9949 - mae: 9.5815 - val_loss: 0.1493 - val_mae: 0.3181\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 355.4198 - mae: 6.7050 - val_loss: 0.2697 - val_mae: 0.5052\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5675 - mae: 0.5161 - val_loss: 0.0187 - val_mae: 0.1156\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0505 - mae: 0.1677 - val_loss: 8.6465 - val_mae: 2.4987\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 252.5001 - mae: 8.0742 - val_loss: 54.5420 - val_mae: 6.4651\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 444.9378 - mae: 11.0155 - val_loss: 7.1123 - val_mae: 2.3683\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 469.3110 - mae: 9.7019 - val_loss: 1544.3069 - val_mae: 34.1608\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.9751 - mae: 8.4390 - val_loss: 61.7532 - val_mae: 6.7761\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 835.5181 - mae: 13.6378 - val_loss: 2.6888 - val_mae: 1.4553\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17.0861 - mae: 1.7203 - val_loss: 17114.5391 - val_mae: 113.5220\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1003.8036 - mae: 11.3850 - val_loss: 0.0104 - val_mae: 0.1005\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 431.2008 - mae: 4.3709 - val_loss: 0.0026 - val_mae: 0.0424\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - mae: 0.0464 - val_loss: 0.0028 - val_mae: 0.0486\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0307 - mae: 0.2731 - val_loss: 30.6972 - val_mae: 4.8285\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 397.3633 - mae: 12.2844 - val_loss: 0.0026 - val_mae: 0.0440\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 499.0439 - mae: 10.0979 - val_loss: 0.0028 - val_mae: 0.0485\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 156.7852 - mae: 2.3947 - val_loss: 0.5559 - val_mae: 0.6604\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 26.0179 - mae: 0.8275 - val_loss: 7567.1309 - val_mae: 75.5192\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 359.5717 - mae: 6.9387 - val_loss: 0.0071 - val_mae: 0.0684\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0211 - mae: 0.0791 - val_loss: 0.0258 - val_mae: 0.1327\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 317.9330 - mae: 6.6928 - val_loss: 0.0748 - val_mae: 0.2323\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0911 - mae: 0.1693 - val_loss: 5.0443e-04 - val_mae: 0.0188\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 133.6899 - mae: 2.9086 - val_loss: 186.9659 - val_mae: 11.8587\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.1892 - mae: 1.8210 - val_loss: 0.0056 - val_mae: 0.0727\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 73.0466 - mae: 2.5482 - val_loss: 0.0178 - val_mae: 0.1225\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1456.0363 - mae: 13.9519 - val_loss: 4.3321e-04 - val_mae: 0.0175\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.9383e-04 - mae: 0.0245 - val_loss: 0.0014 - val_mae: 0.0374\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 276.0359 - mae: 4.4632 - val_loss: 0.9481 - val_mae: 0.8475\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0306 - mae: 0.0646 - val_loss: 3.0748e-05 - val_mae: 0.0049\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 359.8254 - mae: 3.2459 - val_loss: 0.0037 - val_mae: 0.0515\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0124 - mae: 0.0427 - val_loss: 1.5519e-05 - val_mae: 0.0030\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step\n",
      "Metrics:\n",
      "Accuracy: 1.0\n",
      "MSE: 1.551935242051261e-05\n",
      "MAE: 0.003045361891388893\n",
      "RMSE: 0.0039394609301924306\n",
      "F1 Score: 1.0\n",
      "Training Time: 46.7569317817688\n",
      "Inference Time: 0.158416748046875\n",
      "Model type: CNN\n",
      "dataset: addition_dataset_2\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"addition_dataset_2\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=2, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=50,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2987537.5000 - mae: 660.8404 - val_loss: 0.0050 - val_mae: 0.0590\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0051 - mae: 0.0595 - val_loss: 0.0050 - val_mae: 0.0586\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0051 - mae: 0.0595 - val_loss: 0.0050 - val_mae: 0.0584\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0051 - mae: 0.0595 - val_loss: 0.0049 - val_mae: 0.0585\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0050 - mae: 0.0590 - val_loss: 0.0048 - val_mae: 0.0584\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0050 - mae: 0.0590 - val_loss: 0.0048 - val_mae: 0.0581\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0049 - mae: 0.0582 - val_loss: 0.0048 - val_mae: 0.0587\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0048 - mae: 0.0580 - val_loss: 0.0049 - val_mae: 0.0603\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0570 - val_loss: 0.0050 - val_mae: 0.0586\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0048 - mae: 0.0585 - val_loss: 0.0062 - val_mae: 0.0681\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0569 - val_loss: 0.0039 - val_mae: 0.0531\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0581 - val_loss: 0.0037 - val_mae: 0.0503\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0040 - mae: 0.0529 - val_loss: 0.0033 - val_mae: 0.0488\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0046 - mae: 0.0578 - val_loss: 0.0045 - val_mae: 0.0582\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0075 - mae: 0.0667 - val_loss: 0.0364 - val_mae: 0.1844\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7933 - mae: 0.1229 - val_loss: 1242.8165 - val_mae: 30.5784\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 398.4232 - mae: 9.2124 - val_loss: 0.0013 - val_mae: 0.0295\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0656 - mae: 0.1145 - val_loss: 3.3984 - val_mae: 1.5854\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 398.2386 - mae: 9.7302 - val_loss: 5.2902e-04 - val_mae: 0.0191\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0100 - mae: 0.0422 - val_loss: 0.1316 - val_mae: 0.3064\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 136.9287 - mae: 3.7169 - val_loss: 5.8903e-04 - val_mae: 0.0239\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.6815 - mae: 0.3131 - val_loss: 6.2125 - val_mae: 2.1579\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 624.1595 - mae: 10.7817 - val_loss: 4.1586e-04 - val_mae: 0.0175\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0020 - mae: 0.0289 - val_loss: 2.8449e-04 - val_mae: 0.0151\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 21.1778 - mae: 0.4968 - val_loss: 63.2088 - val_mae: 6.9043\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 36.9269 - mae: 2.8564 - val_loss: 6.2453e-05 - val_mae: 0.0068\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.7273 - mae: 0.2803 - val_loss: 2.2215 - val_mae: 1.2959\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 62.5957 - mae: 2.5245 - val_loss: 0.0025 - val_mae: 0.0419\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2739 - mae: 0.2031 - val_loss: 1.7022e-04 - val_mae: 0.0106\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0067 - mae: 0.0237 - val_loss: 0.0019 - val_mae: 0.0393\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.4580 - mae: 2.6153 - val_loss: 0.0030 - val_mae: 0.0468\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9.1162e-04 - mae: 0.0135 - val_loss: 5.7696e-06 - val_mae: 0.0019\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 14.8893 - mae: 0.5352 - val_loss: 72.3846 - val_mae: 7.3827\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 15.1164 - mae: 1.7548 - val_loss: 2.0295e-05 - val_mae: 0.0039\n",
      "Epoch 34: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step\n",
      "Metrics:\n",
      "Accuracy: 1.0\n",
      "MSE: 2.029496849327634e-05\n",
      "MAE: 0.0038605402843095364\n",
      "RMSE: 0.004504993728439179\n",
      "F1 Score: 0.753\n",
      "Training Time: 21.28980827331543\n",
      "Inference Time: 0.11311554908752441\n",
      "Model type: CNN\n",
      "dataset: substraction_dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = \"substraction_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=2, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=50,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 131119760.0000 - mae: 6609.3330 - val_loss: 0.2151 - val_mae: 0.3859\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4426 - mae: 0.4245 - val_loss: 0.2144 - val_mae: 0.3870\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2171 - mae: 0.3874 - val_loss: 0.2141 - val_mae: 0.3877\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2172 - mae: 0.3876 - val_loss: 0.2132 - val_mae: 0.3861\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2218 - mae: 0.3922 - val_loss: 0.2126 - val_mae: 0.3861\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2155 - mae: 0.3861 - val_loss: 0.2115 - val_mae: 0.3851\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2186 - mae: 0.3895 - val_loss: 0.2101 - val_mae: 0.3841\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2137 - mae: 0.3851 - val_loss: 0.2106 - val_mae: 0.3790\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2097 - mae: 0.3821 - val_loss: 0.2116 - val_mae: 0.3912\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2091 - mae: 0.3816 - val_loss: 0.2026 - val_mae: 0.3757\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2089 - mae: 0.3813 - val_loss: 0.2002 - val_mae: 0.3702\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2020 - mae: 0.3739 - val_loss: 0.1944 - val_mae: 0.3660\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2013 - mae: 0.3737 - val_loss: 0.1897 - val_mae: 0.3605\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1943 - mae: 0.3679 - val_loss: 0.1809 - val_mae: 0.3569\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1902 - mae: 0.3643 - val_loss: 0.1761 - val_mae: 0.3568\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1829 - mae: 0.3583 - val_loss: 0.1601 - val_mae: 0.3333\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1691 - mae: 0.3433 - val_loss: 0.1474 - val_mae: 0.3219\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1575 - mae: 0.3329 - val_loss: 0.1342 - val_mae: 0.3018\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1517 - mae: 0.3282 - val_loss: 0.1237 - val_mae: 0.2882\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1984 - mae: 0.3717 - val_loss: 0.0956 - val_mae: 0.2599\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2825 - mae: 0.3604 - val_loss: 37.9275 - val_mae: 5.2278\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 62.4499 - mae: 3.9742 - val_loss: 24.7978 - val_mae: 4.2180\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 541.7724 - mae: 5.8300 - val_loss: 0.2584 - val_mae: 0.4183\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0686 - mae: 0.2165 - val_loss: 0.1006 - val_mae: 0.2685\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0603 - mae: 0.2110 - val_loss: 0.3118 - val_mae: 0.5485\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1293 - mae: 0.5690 - val_loss: 7.7592 - val_mae: 2.3534\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 910.0779 - mae: 12.5401 - val_loss: 0.0200 - val_mae: 0.1181\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0333 - mae: 0.1440 - val_loss: 0.0167 - val_mae: 0.1095\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1417 - mae: 0.2521 - val_loss: 0.1846 - val_mae: 0.3500\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 225.6810 - mae: 5.6353 - val_loss: 0.0514 - val_mae: 0.1855\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 374.9571 - mae: 5.6424 - val_loss: 0.0106 - val_mae: 0.0891\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0174 - mae: 0.1033 - val_loss: 0.0159 - val_mae: 0.1236\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18.1428 - mae: 0.6168 - val_loss: 2323.1143 - val_mae: 41.8023\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 205.3206 - mae: 6.7892 - val_loss: 0.3955 - val_mae: 0.5239\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 261.9207 - mae: 3.3624 - val_loss: 0.0453 - val_mae: 0.1737\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0047 - mae: 0.0530 - val_loss: 0.0022 - val_mae: 0.0408\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0051 - mae: 0.0564 - val_loss: 0.3484 - val_mae: 0.4973\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.4826 - mae: 1.2112 - val_loss: 1741.8396 - val_mae: 36.2116\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 359.6643 - mae: 8.9980 - val_loss: 0.3539 - val_mae: 0.4995\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8529 - mae: 0.4471 - val_loss: 0.0052 - val_mae: 0.0722\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 52.9990 - mae: 1.3459 - val_loss: 0.0024 - val_mae: 0.0471\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 33.7044 - mae: 1.0030 - val_loss: 3.7675 - val_mae: 1.6987\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.6552 - mae: 1.0105 - val_loss: 0.8421 - val_mae: 0.7833\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 84.6855 - mae: 2.0689 - val_loss: 0.0199 - val_mae: 0.1148\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0126 - mae: 0.0780 - val_loss: 0.7473 - val_mae: 0.7380\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 264.5556 - mae: 5.3956 - val_loss: 0.0127 - val_mae: 0.1104\n",
      "Epoch 46: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step\n",
      "Metrics:\n",
      "Accuracy: 1.0\n",
      "MSE: 0.012731780351749522\n",
      "MAE: 0.11044094529002906\n",
      "RMSE: 0.11283519110521115\n",
      "F1 Score: 1.0\n",
      "Training Time: 27.424485683441162\n",
      "Inference Time: 0.11833763122558594\n",
      "Model type: CNN\n",
      "dataset: doubling_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"doubling_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=1, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=50,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 32253505733394432.0000 - mae: 134046632.0000 - val_loss: 29191067349811200.0000 - val_mae: 126391144.0000\n",
      "Epoch 2/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27229261580468224.0000 - mae: 120118152.0000 - val_loss: 17562322063065088.0000 - val_mae: 92637344.0000\n",
      "Epoch 3/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 14447863035592704.0000 - mae: 82255104.0000 - val_loss: 5116885523759104.0000 - val_mae: 49062100.0000\n",
      "Epoch 4/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3957078192291840.0000 - mae: 44889772.0000 - val_loss: 2081823127502848.0000 - val_mae: 39058884.0000\n",
      "Epoch 5/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2045416300347392.0000 - mae: 39065008.0000 - val_loss: 1997908995997696.0000 - val_mae: 39666948.0000\n",
      "Epoch 6/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1983064515280896.0000 - mae: 39328348.0000 - val_loss: 1999276808863744.0000 - val_mae: 39779592.0000\n",
      "Epoch 7/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1995059822067712.0000 - mae: 39502952.0000 - val_loss: 2000068693458944.0000 - val_mae: 39818316.0000\n",
      "Epoch 8/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2000537784418304.0000 - mae: 39561820.0000 - val_loss: 1998464254738432.0000 - val_mae: 39734520.0000\n",
      "Epoch 9/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2018411961909248.0000 - mae: 39711712.0000 - val_loss: 1997999056093184.0000 - val_mae: 39698000.0000\n",
      "Epoch 10/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1995420733538304.0000 - mae: 39396920.0000 - val_loss: 1998868921188352.0000 - val_mae: 39764084.0000\n",
      "Epoch 11/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2020259200499712.0000 - mae: 39713108.0000 - val_loss: 2000805011914752.0000 - val_mae: 39851980.0000\n",
      "Epoch 12/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1996038000869376.0000 - mae: 39474812.0000 - val_loss: 2003162412089344.0000 - val_mae: 39929624.0000\n",
      "Epoch 13/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2002677617655808.0000 - mae: 39733544.0000 - val_loss: 1998527873941504.0000 - val_mae: 39749628.0000\n",
      "Epoch 14/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2021194161192960.0000 - mae: 39795960.0000 - val_loss: 1998201993297920.0000 - val_mae: 39730000.0000\n",
      "Epoch 15/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2018928565944320.0000 - mae: 39764548.0000 - val_loss: 1997638681493504.0000 - val_mae: 39681160.0000\n",
      "Epoch 16/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1970286719139840.0000 - mae: 39124912.0000 - val_loss: 2001331548061696.0000 - val_mae: 39875396.0000\n",
      "Epoch 17/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1999705231851520.0000 - mae: 39517628.0000 - val_loss: 2004611829334016.0000 - val_mae: 39972928.0000\n",
      "Epoch 18/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2002201815810048.0000 - mae: 39688600.0000 - val_loss: 1997408229654528.0000 - val_mae: 39662408.0000\n",
      "Epoch 19/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2014563000123392.0000 - mae: 39600384.0000 - val_loss: 1999779185819648.0000 - val_mae: 39820552.0000\n",
      "Epoch 20/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1996303617753088.0000 - mae: 39527776.0000 - val_loss: 1998399696011264.0000 - val_mae: 39755352.0000\n",
      "Epoch 21/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1995793858822144.0000 - mae: 39448368.0000 - val_loss: 1998357685862400.0000 - val_mae: 39754732.0000\n",
      "Epoch 22/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2025954327134208.0000 - mae: 39846356.0000 - val_loss: 1997649687347200.0000 - val_mae: 39707724.0000\n",
      "Epoch 23/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1981720056299520.0000 - mae: 39263936.0000 - val_loss: 2001266855116800.0000 - val_mae: 39879228.0000\n",
      "Epoch 24/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1989949482074112.0000 - mae: 39410460.0000 - val_loss: 1999568463986688.0000 - val_mae: 39817676.0000\n",
      "Epoch 25/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1986055557349376.0000 - mae: 39344748.0000 - val_loss: 2000986205847552.0000 - val_mae: 39871552.0000\n",
      "Epoch 26/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1989978473103360.0000 - mae: 39449892.0000 - val_loss: 2000394708320256.0000 - val_mae: 39851736.0000\n",
      "Epoch 27/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1998183337033728.0000 - mae: 39536596.0000 - val_loss: 1998564649598976.0000 - val_mae: 39775764.0000\n",
      "Epoch 28/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2005346671394816.0000 - mae: 39637068.0000 - val_loss: 1997224888238080.0000 - val_mae: 39686792.0000\n",
      "Epoch 29/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2012092320186368.0000 - mae: 39692976.0000 - val_loss: 1996942494138368.0000 - val_mae: 39650268.0000\n",
      "Epoch 30/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1998824763555840.0000 - mae: 39556112.0000 - val_loss: 1997545265954816.0000 - val_mae: 39720640.0000\n",
      "Epoch 31/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1992197226364928.0000 - mae: 39462496.0000 - val_loss: 2006227139690496.0000 - val_mae: 40021544.0000\n",
      "Epoch 32/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1996149535801344.0000 - mae: 39503584.0000 - val_loss: 1999682683273216.0000 - val_mae: 39831000.0000\n",
      "Epoch 33/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1985979992768512.0000 - mae: 39434704.0000 - val_loss: 2001763863363584.0000 - val_mae: 39902824.0000\n",
      "Epoch 34/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2022646799663104.0000 - mae: 39807428.0000 - val_loss: 2000526107475968.0000 - val_mae: 39863664.0000\n",
      "Epoch 35/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2010363998502912.0000 - mae: 39627160.0000 - val_loss: 1996713518694400.0000 - val_mae: 39648372.0000\n",
      "Epoch 36/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1999419884961792.0000 - mae: 39504076.0000 - val_loss: 1997079933091840.0000 - val_mae: 39699164.0000\n",
      "Epoch 37/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1995759633301504.0000 - mae: 39529188.0000 - val_loss: 1997672772796416.0000 - val_mae: 39506396.0000\n",
      "Epoch 38/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1977826400010240.0000 - mae: 39212148.0000 - val_loss: 1997728070500352.0000 - val_mae: 39748724.0000\n",
      "Epoch 39/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2017913611485184.0000 - mae: 39703432.0000 - val_loss: 1996516218634240.0000 - val_mae: 39632924.0000\n",
      "Epoch 40/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2000789979529216.0000 - mae: 39428296.0000 - val_loss: 2000819373211648.0000 - val_mae: 39878320.0000\n",
      "Epoch 41/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997156034543616.0000 - mae: 39648576.0000 - val_loss: 1998334063542272.0000 - val_mae: 39784452.0000\n",
      "Epoch 42/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1983479516495872.0000 - mae: 39317776.0000 - val_loss: 1998905562628096.0000 - val_mae: 39810756.0000\n",
      "Epoch 43/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1973523111215104.0000 - mae: 39243528.0000 - val_loss: 1998571763138560.0000 - val_mae: 39797736.0000\n",
      "Epoch 44/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2026120220246016.0000 - mae: 39910724.0000 - val_loss: 1996809752805376.0000 - val_mae: 39698592.0000\n",
      "Epoch 45/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2030430488363008.0000 - mae: 39870460.0000 - val_loss: 1996269929103360.0000 - val_mae: 39615932.0000\n",
      "Epoch 46/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2009851957870592.0000 - mae: 39582516.0000 - val_loss: 1997117916708864.0000 - val_mae: 39726256.0000\n",
      "Epoch 47/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1997294949892096.0000 - mae: 39460384.0000 - val_loss: 1998936432705536.0000 - val_mae: 39817388.0000\n",
      "Epoch 48/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2012949971468288.0000 - mae: 39752520.0000 - val_loss: 1997077651390464.0000 - val_mae: 39727668.0000\n",
      "Epoch 49/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2011594238197760.0000 - mae: 39657616.0000 - val_loss: 1998299703803904.0000 - val_mae: 39793052.0000\n",
      "Epoch 50/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1979746116173824.0000 - mae: 39264424.0000 - val_loss: 1997079396220928.0000 - val_mae: 39731660.0000\n",
      "Epoch 51/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1996415957991424.0000 - mae: 39506932.0000 - val_loss: 1998492440461312.0000 - val_mae: 39803656.0000\n",
      "Epoch 52/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2009795989078016.0000 - mae: 39783140.0000 - val_loss: 1996670300585984.0000 - val_mae: 39707916.0000\n",
      "Epoch 53/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1988712531492864.0000 - mae: 39373892.0000 - val_loss: 1999423106187264.0000 - val_mae: 39841648.0000\n",
      "Epoch 54/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2005663022579712.0000 - mae: 39649648.0000 - val_loss: 1996054375432192.0000 - val_mae: 39651216.0000\n",
      "Epoch 55/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2012080509026304.0000 - mae: 39700640.0000 - val_loss: 1996154501857280.0000 - val_mae: 39669708.0000\n",
      "Epoch 56/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2006675695337472.0000 - mae: 39620060.0000 - val_loss: 1998237560995840.0000 - val_mae: 39798592.0000\n",
      "Epoch 57/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2008159472320512.0000 - mae: 39702700.0000 - val_loss: 1996124437086208.0000 - val_mae: 39673716.0000\n",
      "Epoch 58/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1984377969967104.0000 - mae: 39333176.0000 - val_loss: 1997111742693376.0000 - val_mae: 39747324.0000\n",
      "Epoch 59/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2035255347249152.0000 - mae: 39977844.0000 - val_loss: 1996569368854528.0000 - val_mae: 39511060.0000\n",
      "Epoch 60/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2013381213028352.0000 - mae: 39665280.0000 - val_loss: 1996813779337216.0000 - val_mae: 39733672.0000\n",
      "Epoch 61/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2022226563956736.0000 - mae: 39818348.0000 - val_loss: 1998867310575616.0000 - val_mae: 39828612.0000\n",
      "Epoch 62/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1999495717978112.0000 - mae: 39648560.0000 - val_loss: 1996942359920640.0000 - val_mae: 39744308.0000\n",
      "Epoch 63/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1985851277967360.0000 - mae: 39293168.0000 - val_loss: 1997510100910080.0000 - val_mae: 39774564.0000\n",
      "Epoch 64/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1992522704355328.0000 - mae: 39426256.0000 - val_loss: 1996921556172800.0000 - val_mae: 39746252.0000\n",
      "Epoch 64: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step\n",
      "Metrics:\n",
      "Accuracy: 0.0\n",
      "MSE: 1996921061495296.0\n",
      "MAE: 39746256.35270464\n",
      "RMSE: 44686922.71230249\n",
      "F1 Score: 0.0\n",
      "Training Time: 39.20493173599243\n",
      "Inference Time: 0.12232351303100586\n",
      "Model type: CNN\n",
      "dataset: multiplication_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"multiplication_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=2, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adamW', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=500,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 218329.9531 - mae: 163.9324 - val_loss: 0.5046 - val_mae: 0.6410\n",
      "Epoch 2/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5153 - mae: 0.6417 - val_loss: 0.5707 - val_mae: 0.6596\n",
      "Epoch 3/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5147 - mae: 0.6408 - val_loss: 0.5277 - val_mae: 0.6483\n",
      "Epoch 4/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5250 - mae: 0.6456 - val_loss: 0.5159 - val_mae: 0.6447\n",
      "Epoch 5/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5186 - mae: 0.6394 - val_loss: 0.5048 - val_mae: 0.6411\n",
      "Epoch 6/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5394 - mae: 0.6487 - val_loss: 0.5360 - val_mae: 0.6509\n",
      "Epoch 7/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5430 - mae: 0.6504 - val_loss: 0.5046 - val_mae: 0.6411\n",
      "Epoch 8/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5775 - mae: 0.6621 - val_loss: 0.5254 - val_mae: 0.6464\n",
      "Epoch 9/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6049 - mae: 0.6729 - val_loss: 0.6678 - val_mae: 0.6942\n",
      "Epoch 10/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6715 - mae: 0.6965 - val_loss: 0.8509 - val_mae: 0.7600\n",
      "Epoch 11/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7564 - mae: 0.7269 - val_loss: 0.5236 - val_mae: 0.6461\n",
      "Epoch 11: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step\n",
      "Metrics:\n",
      "Accuracy: 0.33775\n",
      "MSE: 0.5236440536174587\n",
      "MAE: 0.6460512899838754\n",
      "RMSE: 0.7236325404633617\n",
      "F1 Score: 0.3305\n",
      "Training Time: 7.026249885559082\n",
      "Inference Time: 0.11893653869628906\n",
      "Model type: CNN\n",
      "dataset: simple_sin_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"simple_sin_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=1, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=500,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86313.7031 - mae: 100.7560 - val_loss: 0.5012 - val_mae: 0.6361\n",
      "Epoch 2/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5171 - mae: 0.6436 - val_loss: 0.5046 - val_mae: 0.6373\n",
      "Epoch 3/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5311 - mae: 0.6470 - val_loss: 0.5289 - val_mae: 0.6434\n",
      "Epoch 4/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5450 - mae: 0.6543 - val_loss: 0.6412 - val_mae: 0.6819\n",
      "Epoch 5/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5803 - mae: 0.6623 - val_loss: 0.5670 - val_mae: 0.6552\n",
      "Epoch 6/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5707 - mae: 0.6600 - val_loss: 0.4993 - val_mae: 0.6355\n",
      "Epoch 7/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5863 - mae: 0.6680 - val_loss: 0.4980 - val_mae: 0.6346\n",
      "Epoch 8/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.5919 - mae: 0.6680 - val_loss: 0.6783 - val_mae: 0.6923\n",
      "Epoch 9/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7504 - mae: 0.7237 - val_loss: 1.5313 - val_mae: 1.0215\n",
      "Epoch 10/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3107 - mae: 0.8783 - val_loss: 1.0123 - val_mae: 0.8204\n",
      "Epoch 11/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 14.1124 - mae: 1.8892 - val_loss: 15.4525 - val_mae: 3.3847\n",
      "Epoch 12/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 16.0694 - mae: 2.0893 - val_loss: 0.5323 - val_mae: 0.6463\n",
      "Epoch 13/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9.2799 - mae: 1.7909 - val_loss: 6.9773 - val_mae: 2.2642\n",
      "Epoch 14/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 42.2877 - mae: 3.7735 - val_loss: 0.5713 - val_mae: 0.6587\n",
      "Epoch 15/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.5495 - mae: 1.9831 - val_loss: 0.6961 - val_mae: 0.7006\n",
      "Epoch 16/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 103.0802 - mae: 4.0987 - val_loss: 2.6449 - val_mae: 1.3662\n",
      "Epoch 17/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.1412 - mae: 1.2635 - val_loss: 0.6747 - val_mae: 0.6909\n",
      "Epoch 17: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step\n",
      "Metrics:\n",
      "Accuracy: 0.403\n",
      "MSE: 0.6746679342964017\n",
      "MAE: 0.6909392301858818\n",
      "RMSE: 0.8213817226456903\n",
      "F1 Score: 0.33975\n",
      "Training Time: 10.494728565216064\n",
      "Inference Time: 0.11103248596191406\n",
      "Model type: CNN\n",
      "dataset: simple_cos_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"simple_cos_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=1, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=500,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 546036612727832576.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 546036612727832576.0000 - mae: 516242784.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 546036612727832576.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 546036612727832576.0000 - mae: 516242720.0000 - val_loss: 148205629029220352.0000 - val_mae: 192491328.0000\n",
      "Epoch 11: early stopping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Metrics:\n",
      "Accuracy: 0.5\n",
      "MSE: 1.482056353367527e+17\n",
      "MAE: 192491331.06492168\n",
      "RMSE: 384974850.26525134\n",
      "F1 Score: 0.5\n",
      "Training Time: 1.4539759159088135\n",
      "Inference Time: 0.0499880313873291\n",
      "Model type: CNN\n",
      "dataset: exponent_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"exponent_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv',nrows=16)\n",
    "d_set['result']=d_set['result'].astype('int')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=1, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=500,ea=True)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 139082.8750 - mae: 136.2335 - val_loss: 13.9708 - val_mae: 3.2220\n",
      "Epoch 2/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 14.3451 - mae: 3.2550 - val_loss: 13.7366 - val_mae: 3.1814\n",
      "Epoch 3/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.9550 - mae: 3.2037 - val_loss: 13.6191 - val_mae: 3.2115\n",
      "Epoch 4/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.7324 - mae: 3.1782 - val_loss: 13.4927 - val_mae: 3.1097\n",
      "Epoch 5/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.3130 - mae: 3.1385 - val_loss: 15.3596 - val_mae: 3.3478\n",
      "Epoch 6/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.2506 - mae: 3.1374 - val_loss: 11.9869 - val_mae: 2.9378\n",
      "Epoch 7/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 12.5704 - mae: 3.0571 - val_loss: 11.9171 - val_mae: 3.0314\n",
      "Epoch 8/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 11.9668 - mae: 2.9940 - val_loss: 15.0732 - val_mae: 3.3564\n",
      "Epoch 9/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 11.3179 - mae: 2.9098 - val_loss: 8.3305 - val_mae: 2.5004\n",
      "Epoch 10/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 11.3727 - mae: 2.9240 - val_loss: 7.2472 - val_mae: 2.3614\n",
      "Epoch 11/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 13.8919 - mae: 3.1348 - val_loss: 10.7414 - val_mae: 3.1138\n",
      "Epoch 12/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 42.3696 - mae: 4.2639 - val_loss: 15.6857 - val_mae: 3.2308\n",
      "Epoch 13/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 19.5521 - mae: 3.2002 - val_loss: 9.4704 - val_mae: 2.5468\n",
      "Epoch 14/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 96.4376 - mae: 4.4783 - val_loss: 3.2829 - val_mae: 1.5554\n",
      "Epoch 15/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.3368 - mae: 1.9365 - val_loss: 2.5337 - val_mae: 1.4053\n",
      "Epoch 16/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 10.4673 - mae: 2.2769 - val_loss: 2.6305 - val_mae: 1.4151\n",
      "Epoch 17/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 52.8195 - mae: 3.6837 - val_loss: 2.3682 - val_mae: 1.3258\n",
      "Epoch 18/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 47.4318 - mae: 3.0107 - val_loss: 5.7530 - val_mae: 2.3658\n",
      "Epoch 19/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.9314 - mae: 1.7527 - val_loss: 1.1421 - val_mae: 0.9474\n",
      "Epoch 20/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 15.8010 - mae: 2.7522 - val_loss: 23.8694 - val_mae: 4.5977\n",
      "Epoch 21/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 55.5810 - mae: 3.4684 - val_loss: 27.4484 - val_mae: 4.9092\n",
      "Epoch 22/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.8738 - mae: 1.4670 - val_loss: 0.8786 - val_mae: 0.8146\n",
      "Epoch 23/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3571 - mae: 0.9934 - val_loss: 2.2255 - val_mae: 1.1762\n",
      "Epoch 24/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.5708 - mae: 1.0232 - val_loss: 1.8680 - val_mae: 1.3293\n",
      "Epoch 25/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 17.3145 - mae: 2.2678 - val_loss: 138.0148 - val_mae: 10.4854\n",
      "Epoch 26/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.0188 - mae: 5.1758 - val_loss: 1.4263 - val_mae: 0.9255\n",
      "Epoch 27/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 46.7073 - mae: 2.5093 - val_loss: 0.6713 - val_mae: 0.7304\n",
      "Epoch 28/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 96.8316 - mae: 4.1475 - val_loss: 2.3425 - val_mae: 1.1688\n",
      "Epoch 29/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.2060 - mae: 0.8563 - val_loss: 1.9760 - val_mae: 1.0678\n",
      "Epoch 30/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 19.7129 - mae: 1.9982 - val_loss: 0.3862 - val_mae: 0.5354\n",
      "Epoch 31/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8124 - mae: 0.8734 - val_loss: 7.4152 - val_mae: 2.5521\n",
      "Epoch 32/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 140.0951 - mae: 6.6014 - val_loss: 1.4223 - val_mae: 0.9367\n",
      "Epoch 33/500\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8206 - mae: 1.0410 - val_loss: 16.5505 - val_mae: 3.2526\n",
      "Epoch 33: early stopping\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414us/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step\n",
      "Metrics:\n",
      "Accuracy: 0.17475\n",
      "MSE: 16.550530083586462\n",
      "MAE: 3.2526292392980403\n",
      "RMSE: 4.0682342709812644\n",
      "F1 Score: 0.1615\n",
      "Training Time: 20.38612937927246\n",
      "Inference Time: 0.11602044105529785\n",
      "Model type: CNN\n",
      "dataset: natural_log_dataset\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"natural_log_dataset\"\n",
    "d_set = pd.read_csv('./../../Data/'+dataset_name+'.csv')\n",
    "dset_features = d_set.copy()\n",
    "dset_labels = dset_features.pop('result')\n",
    "dset_features = np.array(dset_features)\n",
    "\n",
    "# Reshape features to include timestep dimension\n",
    "# Assuming each row in the CSV file represents a timestep\n",
    "# Reshape to (number_of_samples, timesteps, number_of_features)\n",
    "timesteps = 1  # Assuming each row represents a timestep\n",
    "dset_features_reshaped = dset_features.reshape(-1, timesteps, dset_features.shape[1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dset_features, dset_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure the model with multiple layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=1, activation='relu', input_shape=(dset_features.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model, history, predictions, training_time = train_model(model, X_train, y_train, X_test, y_test,model_name='CNN',dataset_name=dataset_name,total_epoch=500,ea=True,eamonitor='loss')\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_model(model, X_test, y_test,model_name='CNN',dataset_name=dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
